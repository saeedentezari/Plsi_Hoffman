{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "pythonjvsc74a57bd0689805486acef0e487c075666298695e9f8a8f9c117cf3a598c5305b99d19197",
   "display_name": "Python 3.7.3  ('.plsienv': venv)"
  },
  "metadata": {
   "interpreter": {
    "hash": "689805486acef0e487c075666298695e9f8a8f9c117cf3a598c5305b99d19197"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Preparing data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoplist = set('for a of the and to rt'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "source": [
    "## A small corpus"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'Human machine interface for Lab ABC computer applications',\n",
    "    'A survey of user opinion of computer system response time',\n",
    "    'The EPS user interface management system',\n",
    "    'System and human system engineering testing of EPS',\n",
    "    'Relation of user-perceived response time to error measurement',\n",
    "    'The generation of random, binary, unordered trees',\n",
    "    'The intersection graph of paths in trees',\n",
    "    'Graph minors IV: Width of trees and well-quasi-ordering',\n",
    "    'Graph minors: A survey'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('deerwester.txt', 'wb') as f:\n",
    "    pickle.dump(corpus, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating generator object for streaming tweets\n",
    "class Tweets:\n",
    "    def __iter__(self):\n",
    "        for tweet in pickle.load(open('deerwester.txt', 'rb')):\n",
    "            yield tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 [(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1)]\n1 [(7, 1), (8, 1), (9, 1), (5, 1), (10, 1), (11, 1), (12, 1)]\n2 [(13, 1), (8, 1), (2, 1), (14, 1), (10, 1)]\n3 [(10, 2), (0, 1), (15, 1), (16, 1), (13, 1)]\n4 [(17, 1), (8, 1), (18, 1), (11, 1), (12, 1), (19, 1), (20, 1)]\n5 [(21, 1), (22, 1), (23, 1), (24, 1), (25, 1)]\n6 [(26, 1), (27, 1), (28, 1), (29, 1), (25, 1)]\n7 [(27, 1), (30, 1), (31, 1), (32, 1), (25, 1), (33, 1), (34, 1), (35, 1)]\n8 [(27, 1), (30, 1), (7, 1)]\n"
     ]
    }
   ],
   "source": [
    "# streaming corpus and storing documents in bow representation\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "tweets = Tweets()\n",
    "token2id = {}\n",
    "# token2id : dict of (token(str), tokenId(int))\n",
    "idf = defaultdict(int)\n",
    "# idf: dict of (tokenId, frequency of tokenId in corpus)\n",
    "docs2bow = []\n",
    "# docs2bow: list of [doc2bow]\n",
    "# doc2bow: list of (tokenIds in doc, frequency of tokenId in doc)\n",
    "for docno, tweet in enumerate(tweets):\n",
    "    # lowering tweets and removing punctuations from it, then splitting\n",
    "    document = re.sub(r'[-,:;|.!?*()+&/~<>=\"]', ' ', tweet.lower()).split()\n",
    "    counter = defaultdict(int)\n",
    "    # counter: dict of (tokenIds in doc, frequency of tokenId in doc)\n",
    "    for word in document:\n",
    "        if word in stoplist: continue   # check word by stoplist\n",
    "        if word not in token2id: token2id[word] = len(token2id) # add word as a token if seen for the first time\n",
    "        counter[word] += 1\n",
    "        idf[token2id[word]] += 1\n",
    "    # creating doc2bow for this doc\n",
    "    doc2bow = [(token2id[word], freq) for word, freq in counter.items()]\n",
    "    print(docno, doc2bow)\n",
    "    # append doc2bow to docs2bow\n",
    "    docs2bow.append(doc2bow)"
   ]
  },
  {
   "source": [
    "## A user timeline corpus"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 840,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy as tw\n",
    "\n",
    "# load keys\n",
    "with open('keys.txt', 'rb') as f:\n",
    "    keys = pickle.load(f)\n",
    "# define keys\n",
    "consumer_key = keys['consumer_key']\n",
    "consumer_secret = keys['consumer_secret']\n",
    "access_token = keys['access_token']\n",
    "access_token_secret = keys['access_token_secret']\n",
    "# authenticate and create api object\n",
    "auth = tw.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tw.API(auth, wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 838,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator object for iterating through user timeline\n",
    "class Tweets():\n",
    "    def __init__(self, pagination_num=3):\n",
    "        self.pagination_num = pagination_num\n",
    "        self.cursor = tw.Cursor(api.user_timeline, id=\"unicef\", # id = \"indykaila\"\n",
    "                              exclude_replies=True,\n",
    "                              include_rts=True,\n",
    "                              tweet_mode='extended').pages(self.pagination_num)\n",
    "    def __iter__(self):\n",
    "        for page in self.cursor:\n",
    "            for status in page:\n",
    "                yield status.full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 853,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n",
      "939\n",
      "940\n",
      "941\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "950\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "960\n",
      "961\n",
      "962\n",
      "963\n",
      "964\n",
      "965\n",
      "966\n",
      "967\n",
      "968\n",
      "969\n",
      "970\n",
      "971\n",
      "972\n",
      "973\n",
      "974\n",
      "975\n",
      "976\n",
      "977\n",
      "978\n",
      "979\n",
      "980\n",
      "981\n",
      "982\n",
      "983\n",
      "984\n",
      "985\n",
      "986\n",
      "987\n",
      "988\n",
      "989\n",
      "990\n",
      "991\n",
      "992\n",
      "993\n",
      "994\n",
      "995\n",
      "996\n",
      "997\n",
      "998\n",
      "999\n",
      "1000\n",
      "1001\n",
      "1002\n",
      "1003\n",
      "1004\n",
      "1005\n",
      "1006\n",
      "1007\n",
      "1008\n",
      "1009\n",
      "1010\n",
      "1011\n",
      "1012\n",
      "1013\n",
      "1014\n",
      "1015\n",
      "1016\n",
      "1017\n",
      "1018\n",
      "1019\n",
      "1020\n",
      "1021\n",
      "1022\n",
      "1023\n",
      "1024\n",
      "1025\n",
      "1026\n",
      "1027\n",
      "1028\n",
      "1029\n",
      "1030\n",
      "1031\n",
      "1032\n",
      "1033\n",
      "1034\n",
      "1035\n",
      "1036\n",
      "1037\n",
      "1038\n",
      "1039\n",
      "1040\n",
      "1041\n",
      "1042\n",
      "1043\n",
      "1044\n",
      "1045\n",
      "1046\n",
      "1047\n",
      "1048\n",
      "1049\n",
      "1050\n",
      "1051\n",
      "1052\n",
      "1053\n",
      "1054\n",
      "1055\n",
      "1056\n",
      "1057\n",
      "1058\n",
      "1059\n",
      "1060\n",
      "1061\n",
      "1062\n",
      "1063\n",
      "1064\n",
      "1065\n",
      "1066\n",
      "1067\n",
      "1068\n",
      "1069\n",
      "1070\n",
      "1071\n",
      "1072\n",
      "1073\n",
      "1074\n",
      "1075\n",
      "1076\n",
      "1077\n",
      "1078\n",
      "1079\n",
      "1080\n",
      "1081\n",
      "1082\n",
      "1083\n",
      "1084\n",
      "1085\n",
      "1086\n",
      "1087\n",
      "1088\n",
      "1089\n",
      "1090\n",
      "1091\n",
      "1092\n",
      "1093\n",
      "1094\n",
      "1095\n",
      "1096\n",
      "1097\n",
      "1098\n",
      "1099\n",
      "1100\n",
      "1101\n",
      "1102\n",
      "1103\n",
      "1104\n",
      "1105\n",
      "1106\n",
      "1107\n",
      "1108\n",
      "1109\n",
      "1110\n",
      "1111\n",
      "1112\n",
      "1113\n",
      "1114\n",
      "1115\n",
      "1116\n",
      "1117\n",
      "1118\n",
      "1119\n",
      "1120\n",
      "1121\n",
      "1122\n",
      "1123\n",
      "1124\n",
      "1125\n",
      "1126\n",
      "1127\n",
      "1128\n",
      "1129\n",
      "1130\n",
      "1131\n",
      "1132\n",
      "1133\n",
      "1134\n",
      "1135\n",
      "1136\n",
      "1137\n",
      "1138\n",
      "1139\n",
      "1140\n",
      "1141\n",
      "1142\n",
      "1143\n",
      "1144\n",
      "1145\n",
      "1146\n",
      "1147\n",
      "1148\n",
      "1149\n",
      "1150\n",
      "1151\n",
      "1152\n",
      "1153\n",
      "1154\n",
      "1155\n",
      "1156\n",
      "1157\n",
      "1158\n",
      "1159\n",
      "1160\n",
      "1161\n",
      "1162\n",
      "1163\n"
     ]
    }
   ],
   "source": [
    "# streaming corpus and storing documents in bow representation\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "tweets = Tweets()\n",
    "token2id = {}\n",
    "idf = defaultdict(int)\n",
    "docs2bow = []\n",
    "for docno, tweet in enumerate(tweets):\n",
    "    # remove links from tweets\n",
    "    tweet = re.sub(r'\\bhttps:\\S+', '', tweet.lower())\n",
    "    # print(tweet)\n",
    "    document = re.sub(r'[-,:;|.!?*()+&/~<>=\"]', ' ', tweet).split()\n",
    "    # print(document)\n",
    "    counter = defaultdict(int)\n",
    "    for word in document:\n",
    "        if word in stoplist: continue\n",
    "        if word not in token2id: token2id[word] = len(token2id)\n",
    "        counter[word] += 1\n",
    "        idf[token2id[word]] += 1\n",
    "    doc2bow = [(token2id[word], freq) for word, freq in counter.items()]\n",
    "    # print(docno, doc2bow)\n",
    "    print(docno)\n",
    "    docs2bow.append(doc2bow)"
   ]
  },
  {
   "source": [
    "## Corpus read"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "len(token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'human': 0,\n",
       " 'machine': 1,\n",
       " 'interface': 2,\n",
       " 'lab': 3,\n",
       " 'abc': 4,\n",
       " 'computer': 5,\n",
       " 'applications': 6,\n",
       " 'survey': 7,\n",
       " 'user': 8,\n",
       " 'opinion': 9,\n",
       " 'system': 10,\n",
       " 'response': 11,\n",
       " 'time': 12,\n",
       " 'eps': 13,\n",
       " 'management': 14,\n",
       " 'engineering': 15,\n",
       " 'testing': 16,\n",
       " 'relation': 17,\n",
       " 'perceived': 18,\n",
       " 'error': 19,\n",
       " 'measurement': 20,\n",
       " 'generation': 21,\n",
       " 'random': 22,\n",
       " 'binary': 23,\n",
       " 'unordered': 24,\n",
       " 'trees': 25,\n",
       " 'intersection': 26,\n",
       " 'graph': 27,\n",
       " 'paths': 28,\n",
       " 'in': 29,\n",
       " 'minors': 30,\n",
       " 'iv': 31,\n",
       " 'width': 32,\n",
       " 'well': 33,\n",
       " 'quasi': 34,\n",
       " 'ordering': 35}"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "len(idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {0: 2,\n",
       "             1: 1,\n",
       "             2: 2,\n",
       "             3: 1,\n",
       "             4: 1,\n",
       "             5: 2,\n",
       "             6: 1,\n",
       "             7: 2,\n",
       "             8: 3,\n",
       "             9: 1,\n",
       "             10: 4,\n",
       "             11: 2,\n",
       "             12: 2,\n",
       "             13: 2,\n",
       "             14: 1,\n",
       "             15: 1,\n",
       "             16: 1,\n",
       "             17: 1,\n",
       "             18: 1,\n",
       "             19: 1,\n",
       "             20: 1,\n",
       "             21: 1,\n",
       "             22: 1,\n",
       "             23: 1,\n",
       "             24: 1,\n",
       "             25: 3,\n",
       "             26: 1,\n",
       "             27: 3,\n",
       "             28: 1,\n",
       "             29: 1,\n",
       "             30: 2,\n",
       "             31: 1,\n",
       "             32: 1,\n",
       "             33: 1,\n",
       "             34: 1,\n",
       "             35: 1})"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "len(docs2bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1)],\n",
       " [(7, 1), (8, 1), (9, 1), (5, 1), (10, 1), (11, 1), (12, 1)],\n",
       " [(13, 1), (8, 1), (2, 1), (14, 1), (10, 1)],\n",
       " [(10, 2), (0, 1), (15, 1), (16, 1), (13, 1)],\n",
       " [(17, 1), (8, 1), (18, 1), (11, 1), (12, 1), (19, 1), (20, 1)],\n",
       " [(21, 1), (22, 1), (23, 1), (24, 1), (25, 1)],\n",
       " [(26, 1), (27, 1), (28, 1), (29, 1), (25, 1)],\n",
       " [(27, 1), (30, 1), (31, 1), (32, 1), (25, 1), (33, 1), (34, 1), (35, 1)],\n",
       " [(27, 1), (30, 1), (7, 1)]]"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "docs2bow"
   ]
  },
  {
   "source": [
    "## Filter once words"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter once words\n",
    "bad_ids = set(tokenid for tokenid, freq in idf.items() if freq == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "len(bad_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{1,\n",
       " 3,\n",
       " 4,\n",
       " 6,\n",
       " 9,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 26,\n",
       " 28,\n",
       " 29,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35}"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "bad_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update token2id and idf which filtered once words\n",
    "token2id = {token: tokenid for token, tokenid in token2id.items() if idf[tokenid] > 1}\n",
    "idf = {tokenid: freq for tokenid, freq in idf.items() if freq > 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "len(token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'human': 0,\n",
       " 'interface': 2,\n",
       " 'computer': 5,\n",
       " 'survey': 7,\n",
       " 'user': 8,\n",
       " 'system': 10,\n",
       " 'response': 11,\n",
       " 'time': 12,\n",
       " 'eps': 13,\n",
       " 'trees': 25,\n",
       " 'graph': 27,\n",
       " 'minors': 30}"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{0: 2, 2: 2, 5: 2, 7: 2, 8: 3, 10: 4, 11: 2, 12: 2, 13: 2, 25: 3, 27: 3, 30: 2}"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idmap: maps old tokenIds to new ordered tokenIds\n",
    "idmap = dict(zip(sorted(token2id.values()), range(len(token2id))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "len(idmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{0: 0,\n",
       " 2: 1,\n",
       " 5: 2,\n",
       " 7: 3,\n",
       " 8: 4,\n",
       " 10: 5,\n",
       " 11: 6,\n",
       " 12: 7,\n",
       " 13: 8,\n",
       " 25: 9,\n",
       " 27: 10,\n",
       " 30: 11}"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "idmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note this cell is one time run\n",
    "# compactify token2id and idf\n",
    "token2id = {token: idmap[tokenid] for token, tokenid in token2id.items()}\n",
    "idf = {idmap[tokenid]: freq for tokenid, freq in idf.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'human': 0,\n",
       " 'interface': 1,\n",
       " 'computer': 2,\n",
       " 'survey': 3,\n",
       " 'user': 4,\n",
       " 'system': 5,\n",
       " 'response': 6,\n",
       " 'time': 7,\n",
       " 'eps': 8,\n",
       " 'trees': 9,\n",
       " 'graph': 10,\n",
       " 'minors': 11}"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{0: 2, 1: 2, 2: 2, 3: 2, 4: 3, 5: 4, 6: 2, 7: 2, 8: 2, 9: 3, 10: 3, 11: 2}"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1)],\n",
       " [(7, 1), (8, 1), (9, 1), (5, 1), (10, 1), (11, 1), (12, 1)],\n",
       " [(13, 1), (8, 1), (2, 1), (14, 1), (10, 1)],\n",
       " [(10, 2), (0, 1), (15, 1), (16, 1), (13, 1)],\n",
       " [(17, 1), (8, 1), (18, 1), (11, 1), (12, 1), (19, 1), (20, 1)],\n",
       " [(21, 1), (22, 1), (23, 1), (24, 1), (25, 1)],\n",
       " [(26, 1), (27, 1), (28, 1), (29, 1), (25, 1)],\n",
       " [(27, 1), (30, 1), (31, 1), (32, 1), (25, 1), (33, 1), (34, 1), (35, 1)],\n",
       " [(27, 1), (30, 1), (7, 1)]]"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "# token2id changed, but docs2bow still has the same old tokenIds\n",
    "docs2bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rebuild docs2bow based on new token2id\n",
    "docs2bow = [\n",
    "    [(idmap[tokenid], docfreq) for tokenid, docfreq in doc2bow if tokenid not in bad_ids]\n",
    "    for doc2bow in docs2bow\n",
    "]"
   ]
  },
  {
   "source": [
    "## `docs2bow` ready to use"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1)],\n",
       " [(3, 1), (4, 1), (2, 1), (5, 1), (6, 1), (7, 1)],\n",
       " [(8, 1), (4, 1), (1, 1), (5, 1)],\n",
       " [(5, 2), (0, 1), (8, 1)],\n",
       " [(4, 1), (6, 1), (7, 1)],\n",
       " [(9, 1)],\n",
       " [(10, 1), (9, 1)],\n",
       " [(10, 1), (11, 1), (9, 1)],\n",
       " [(10, 1), (11, 1), (3, 1)]]"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "docs2bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the corpus in bow representation\n",
    "with open('corpus2bow.txt', 'wb') as f:\n",
    "    pickle.dump(docs2bow, f)"
   ]
  },
  {
   "source": [
    "## Additional"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 2., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1.]])"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "# not necessary, just n.shape needed for further uses\n",
    "# note that we don't use n(d,w) matrix in computations, n(d,w) is presented in docs2bow as ndw that you'll see\n",
    "# creating words2bod by docs2bow -> n -> n.T -> words2bod\n",
    "# words2bod shows each word appeared in which docs\n",
    "import numpy as np\n",
    "# n: numpy array of n[d][w] = n(d,w)\n",
    "# d = document number, w = word's tokenId\n",
    "n = np.zeros((len(docs2bow), len(idf)))\n",
    "for docno, doc2bow in enumerate(docs2bow):\n",
    "    for tokenid, docfreq in doc2bow:\n",
    "        n[docno, tokenid] += docfreq\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(9, 12)"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "n.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 1., 1., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 1., 1., 2., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 1., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 1., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 1.]])"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "n.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "words2bod = [\n",
    "    [(tokenid, int(docfreq)) for tokenid, docfreq in enumerate(rows) if docfreq != 0]\n",
    "    for rows in n.T\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[(0, 1), (3, 1)],\n",
       " [(0, 1), (2, 1)],\n",
       " [(0, 1), (1, 1)],\n",
       " [(1, 1), (8, 1)],\n",
       " [(1, 1), (2, 1), (4, 1)],\n",
       " [(1, 1), (2, 1), (3, 2)],\n",
       " [(1, 1), (4, 1)],\n",
       " [(1, 1), (4, 1)],\n",
       " [(2, 1), (3, 1)],\n",
       " [(5, 1), (6, 1), (7, 1)],\n",
       " [(6, 1), (7, 1), (8, 1)],\n",
       " [(7, 1), (8, 1)]]"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "words2bod"
   ]
  },
  {
   "source": [
    "# PLSI model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Parameters and likelihood"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 2   # K: number of topics considered, namely the size of latent semantic set Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import rand\n",
    "\n",
    "def random_init_pars(K, nshape):\n",
    "    N, M = nshape   # N = number of documents, M = number of tokens\n",
    "    Pz = rand(K); Pz /= sum(Pz) # P(z)\n",
    "    Pd_z = rand(N, K); Pd_z /= Pd_z.sum(axis=0) # P(d|z)\n",
    "    Pw_z = rand(M, K); Pw_z /= Pw_z.sum(axis=0) # P(w|z)\n",
    "    pars = Pz, Pd_z, Pw_z   # pack parameters in a variable called pars\n",
    "    return pars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(9, 12)"
      ]
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "n.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(array([0.4001385, 0.5998615]),\n",
       " array([[0.11300014, 0.13423582],\n",
       "        [0.15208975, 0.14145356],\n",
       "        [0.11296352, 0.14415165],\n",
       "        [0.10676998, 0.05131162],\n",
       "        [0.14655433, 0.0403815 ],\n",
       "        [0.02229351, 0.17183628],\n",
       "        [0.11544259, 0.166549  ],\n",
       "        [0.14183071, 0.10509881],\n",
       "        [0.08905547, 0.04498175]]),\n",
       " array([[0.01186479, 0.08050633],\n",
       "        [0.12389336, 0.03654783],\n",
       "        [0.03002695, 0.18772072],\n",
       "        [0.08509484, 0.13924188],\n",
       "        [0.17923298, 0.00066867],\n",
       "        [0.12707695, 0.13626144],\n",
       "        [0.17833826, 0.14977082],\n",
       "        [0.0883424 , 0.14072519],\n",
       "        [0.03751731, 0.06700934],\n",
       "        [0.09520133, 0.02886329],\n",
       "        [0.0124025 , 0.02764159],\n",
       "        [0.03100834, 0.0050429 ]]))"
      ]
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "random_init_pars(K, n.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood(pars, docs2bow):\n",
    "    Pz, Pd_z, Pw_z = pars   # unpack parameters\n",
    "    L = 0\n",
    "    # iterate over data in docs2bow and calculate prob of co-occur for them, based on pars\n",
    "    for d, doc2bow in enumerate(docs2bow):\n",
    "        for w, ndw in doc2bow:\n",
    "            Pcocur = sum(Pz[:] * Pd_z[d,:] * Pw_z[w, :])    # P(d,w)\n",
    "            # adding up all log-likelihood terms\n",
    "            L += ndw * np.log(Pcocur)\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "-153.1750646575668"
      ]
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "source": [
    "likelihood(random_init_pars(K, n.shape), docs2bow)"
   ]
  },
  {
   "source": [
    "## EM"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expectation step\n",
    "def Estep(pars, docs2bow):  # no necessity to pass docs2bow (data) to Estep, but it'll help to decrease computations\n",
    "    Pz, Pd_z, Pw_z = pars\n",
    "    posters = np.zeros((len(Pz), len(Pd_z), len(Pw_z)))\n",
    "    # posters could be an attribute and no need to reset to zeros because it's not accumulative\n",
    "    # iterate through data and calculate posteriors just for seen pairs of (d, w)\n",
    "    # so unseen posteriors left to be zero\n",
    "    for z in range(len(Pz)):\n",
    "        for d, doc2bow in enumerate(docs2bow):\n",
    "            for w, ndw in doc2bow:\n",
    "                posters[z, d, w] = Pz[z] * Pd_z[d, z] * Pw_z[w, z]\n",
    "    # normalization\n",
    "    posters /= posters.sum(axis=0) + 1e-16  # a tiny number added just to avoid dividing by zero error for unseen (d, w)s\n",
    "    return posters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[[0.07513966, 0.48936232, 0.72259835, 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.61876912, 0.42224057, 0.44602618,\n",
       "         0.23384216, 0.20328817, 0.05547501, 0.        , 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.44267243, 0.        , 0.        , 0.51713187,\n",
       "         0.28875307, 0.        , 0.        , 0.65852609, 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.09475946, 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.38692732, 0.        , 0.        , 0.7498728 , 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.86712669,\n",
       "         0.        , 0.67407196, 0.32251873, 0.        , 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.74613442,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.88719546,\n",
       "         0.91509502, 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.56472056,\n",
       "         0.64001501, 0.84904462],\n",
       "        [0.        , 0.        , 0.        , 0.86570779, 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.9094047 , 0.96947137]],\n",
       "\n",
       "       [[0.92486034, 0.51063768, 0.27740165, 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.38123088, 0.57775943, 0.55397382,\n",
       "         0.76615784, 0.79671183, 0.94452499, 0.        , 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.55732757, 0.        , 0.        , 0.48286813,\n",
       "         0.71124693, 0.        , 0.        , 0.34147391, 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.90524054, 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.61307268, 0.        , 0.        , 0.2501272 , 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.13287331,\n",
       "         0.        , 0.32592804, 0.67748127, 0.        , 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.25386558,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.11280454,\n",
       "         0.08490498, 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.43527944,\n",
       "         0.35998499, 0.15095538],\n",
       "        [0.        , 0.        , 0.        , 0.13429221, 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.0905953 , 0.03052863]]])"
      ]
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": [
    "posters = Estep(random_init_pars(K, n.shape), docs2bow)\n",
    "posters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(2, 9, 12)"
      ]
     },
     "metadata": {},
     "execution_count": 55
    }
   ],
   "source": [
    "posters.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1.]])"
      ]
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "source": [
    "# is posters normalized?\n",
    "posters.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximization step\n",
    "def Mstep(posters, docs2bow):\n",
    "    # re-estimation of the parameters by posteriors calculated in E-step based on parameters\n",
    "    K, N, M = posters.shape  # K, N, M could be in an attribute self.archit\n",
    "    rePz, rePd_z, rePw_z = np.zeros(K), np.zeros((N, K)), np.zeros((M, K))\n",
    "    # repars should reset to zeros in each M-step because they'd be calculated accumulatively\n",
    "    # iterate over data and add up terms n(d,w) * poster(z|d,w) to associated repars\n",
    "    for z in range(K):\n",
    "        for d, doc2bow in enumerate(docs2bow):\n",
    "            for w, ndw in doc2bow:\n",
    "                rePz[z] += ndw * posters[z, d, w]\n",
    "                rePd_z[d, z] += ndw * posters[z, d, w]\n",
    "                rePw_z[w, z] += ndw * posters[z, d, w]\n",
    "    # normalization\n",
    "    rePz /= sum(rePz)\n",
    "    rePd_z /= rePd_z.sum(axis=0)\n",
    "    rePw_z /= rePw_z.sum(axis=0)\n",
    "    repars = rePz, rePd_z, rePw_z   # pack re-estimated parameters in repars and return it\n",
    "    return repars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(array([0.24922167, 0.75077833]), array([[0.09677253, 0.1602031 ],\n       [0.00775021, 0.21583797],\n       [0.00428237, 0.01857995],\n       [0.09641279, 0.15206515],\n       [0.24054326, 0.01789933],\n       [0.06312533, 0.00745956],\n       [0.09122164, 0.07598723],\n       [0.16340418, 0.08003833],\n       [0.2364877 , 0.2719294 ]]), array([[0.10680976, 0.03685408],\n       [0.11841676, 0.0282922 ],\n       [0.01953122, 0.13461726],\n       [0.1143877 , 0.08770452],\n       [0.07620412, 0.0837982 ],\n       [0.07945055, 0.13575207],\n       [0.04408571, 0.10852351],\n       [0.10540922, 0.07066863],\n       [0.00814478, 0.11543443],\n       [0.1067055 , 0.10708165],\n       [0.11714257, 0.04682082],\n       [0.10371211, 0.04445263]]))\n-141.13914367519305\n(array([0.29152057, 0.70847943]), array([[0.10079388, 0.10454049],\n       [0.00673905, 0.28925608],\n       [0.04209003, 0.17736708],\n       [0.07249001, 0.16485829],\n       [0.27394979, 0.0332914 ],\n       [0.08715156, 0.01281094],\n       [0.09267565, 0.05920942],\n       [0.19454416, 0.06596472],\n       [0.12956587, 0.09270158]]), array([[0.08829011, 0.06101396],\n       [0.08266542, 0.06332837],\n       [0.00354818, 0.09588303],\n       [0.03416526, 0.08328491],\n       [0.10385686, 0.10328015],\n       [0.03183347, 0.18158738],\n       [0.07679407, 0.06574428],\n       [0.10489801, 0.05418025],\n       [0.00236594, 0.09636948],\n       [0.16845333, 0.07670041],\n       [0.18306495, 0.07068812],\n       [0.12006442, 0.04793967]]))\n-129.40822579332698\n"
     ]
    }
   ],
   "source": [
    "# just one EM-step\n",
    "pars = random_init_pars(K, n.shape)\n",
    "print(pars)\n",
    "print(likelihood(pars, docs2bow))\n",
    "# EM\n",
    "posters = Estep(pars, docs2bow)\n",
    "repars = Mstep(posters, docs2bow)\n",
    "print(repars)\n",
    "print(likelihood(repars, docs2bow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.0\n[1. 1.]\n[1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# check whether parameters remain normalized\n",
    "Pz, Pd_z, Pw_z = repars\n",
    "print(Pz.sum(axis=0))\n",
    "print(Pd_z.sum(axis=0))\n",
    "print(Pw_z.sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expectation Maximization steps\n",
    "def EMsteps(runtimes, pars, docs2bow):\n",
    "    print(pars)\n",
    "    print(likelihood(pars, docs2bow))\n",
    "    for runtime in range(runtimes):\n",
    "        posters = Estep(pars, docs2bow)\n",
    "        pars = Mstep(posters, docs2bow)\n",
    "        print(likelihood(pars, docs2bow))\n",
    "    print(pars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(array([0.49919619, 0.50080381]), array([[0.00540957, 0.12289335],\n       [0.22141974, 0.17191716],\n       [0.01317461, 0.12508   ],\n       [0.12972748, 0.14250206],\n       [0.18448682, 0.01071087],\n       [0.0221535 , 0.16361161],\n       [0.10217125, 0.05755784],\n       [0.15699478, 0.17376754],\n       [0.16446224, 0.03195957]]), array([[0.01878284, 0.12819708],\n       [0.08641681, 0.00456281],\n       [0.14649302, 0.00887281],\n       [0.08354953, 0.15345632],\n       [0.1111021 , 0.13396   ],\n       [0.16075068, 0.09740126],\n       [0.02900145, 0.08714917],\n       [0.0235575 , 0.02555719],\n       [0.06534193, 0.11582632],\n       [0.09665643, 0.08438132],\n       [0.06355762, 0.130665  ],\n       [0.1147901 , 0.02997071]]))\n-141.17278431330504\n-130.70270083840904\n-127.63669856803766\n-124.14865627778187\n-122.03110881423996\n-121.34445581430512\n-121.06149326921965\n-120.83685580360137\n-120.59288899356928\n-120.33518770727943\n-120.1202055580319\n-119.98218733077078\n-119.90025916872264\n-119.84419359641178\n-119.79989405210817\n-119.76328923713484\n-119.732902101452\n-119.70699024182407\n-119.68285212248482\n-119.65626518148254\n-119.61957376209651\n(array([0.51726812, 0.48273188]), array([[1.49004657e-016, 2.14297584e-001],\n       [2.63289562e-001, 1.46468998e-001],\n       [3.31396964e-009, 2.85730109e-001],\n       [4.90184067e-032, 2.85730112e-001],\n       [1.99989661e-001, 7.87985990e-014],\n       [3.41506852e-003, 6.77731342e-002],\n       [1.33326383e-001, 6.21155504e-008],\n       [1.99989661e-001, 1.91172271e-010],\n       [1.99989661e-001, 2.61674894e-127]]), array([[8.83820425e-117, 1.42865056e-001],\n       [1.27127772e-072, 1.42865056e-001],\n       [1.52594116e-002, 1.26513936e-001],\n       [1.33326441e-001, 1.57658262e-016],\n       [1.14558771e-001, 9.15428993e-002],\n       [1.44942300e-004, 2.85574800e-001],\n       [1.33326441e-001, 9.62833542e-017],\n       [1.33326441e-001, 5.05898035e-017],\n       [5.53781214e-074, 1.42865056e-001],\n       [1.36741451e-001, 6.77731965e-002],\n       [1.99989661e-001, 4.12679405e-043],\n       [1.33326441e-001, 2.09171347e-063]]))\n"
     ]
    }
   ],
   "source": [
    "# EM-step several times\n",
    "pars = random_init_pars(K, n.shape)\n",
    "EMsteps(20, pars, docs2bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'human': 0,\n",
       " 'interface': 1,\n",
       " 'computer': 2,\n",
       " 'survey': 3,\n",
       " 'user': 4,\n",
       " 'system': 5,\n",
       " 'response': 6,\n",
       " 'time': 7,\n",
       " 'eps': 8,\n",
       " 'trees': 9,\n",
       " 'graph': 10,\n",
       " 'minors': 11}"
      ]
     },
     "metadata": {},
     "execution_count": 63
    }
   ],
   "source": [
    "# now you can judge the result. We'll do it formally later\n",
    "token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['Human machine interface for Lab ABC computer applications',\n",
       " 'A survey of user opinion of computer system response time',\n",
       " 'The EPS user interface management system',\n",
       " 'System and human system engineering testing of EPS',\n",
       " 'Relation of user-perceived response time to error measurement',\n",
       " 'The generation of random, binary, unordered trees',\n",
       " 'The intersection graph of paths in trees',\n",
       " 'Graph minors IV: Width of trees and well-quasi-ordering',\n",
       " 'Graph minors: A survey']"
      ]
     },
     "metadata": {},
     "execution_count": 64
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "source": [
    "## TEM"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tempered Expectation step with control parameter beta\n",
    "def TEstep(beta, pars, docs2bow):\n",
    "    Pz, Pd_z, Pw_z = pars\n",
    "    posters = np.zeros((len(Pz), len(Pd_z), len(Pw_z)))\n",
    "    for z in range(len(Pz)):\n",
    "        for d, doc2bow in enumerate(docs2bow):\n",
    "            for w, ndw in doc2bow:\n",
    "                posters[z, d, w] = (Pz[z] * Pd_z[d, z] * Pw_z[w, z])**beta  # beta\n",
    "    posters /= posters.sum(axis=0) + 1e-16\n",
    "    return posters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tempered Maximization step\n",
    "def TMstep(posters, docs2bow):  # note beta has no role in TM-step. it played its role in TE-step\n",
    "    K, N, M = posters.shape\n",
    "    rePz, rePd_z, rePw_z = np.zeros(K), np.zeros((N, K)), np.zeros((M, K))\n",
    "    for z in range(K):\n",
    "        for d, doc2bow in enumerate(docs2bow):\n",
    "            for w, ndw in doc2bow:\n",
    "                rePz[z] += ndw * posters[z, d, w]\n",
    "                rePd_z[d, z] += ndw * posters[z, d, w]\n",
    "                rePw_z[w, z] += ndw * posters[z, d, w]\n",
    "    rePz /= sum(rePz)\n",
    "    rePd_z /= rePd_z.sum(axis=0)\n",
    "    rePw_z /= rePw_z.sum(axis=0)\n",
    "    repars = rePz, rePd_z, rePw_z\n",
    "    return repars"
   ]
  },
  {
   "source": [
    "## Split data to train and held-out"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "metadata": {},
     "execution_count": 67
    }
   ],
   "source": [
    "len(docs2bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1)],\n",
       " [(3, 1), (4, 1), (2, 1), (5, 1), (6, 1), (7, 1)],\n",
       " [(8, 1), (4, 1), (1, 1), (5, 1)],\n",
       " [(5, 2), (0, 1), (8, 1)],\n",
       " [(4, 1), (6, 1), (7, 1)],\n",
       " [(9, 1)],\n",
       " [(10, 1), (9, 1)],\n",
       " [(10, 1), (11, 1), (9, 1)],\n",
       " [(10, 1), (11, 1), (3, 1)]]"
      ]
     },
     "metadata": {},
     "execution_count": 68
    }
   ],
   "source": [
    "docs2bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over the corpus and randomly erase words\n",
    "# erased words will be writed in held-out corpus\n",
    "# unerased words remain in corpus as training corpus\n",
    "from numpy.random import randint\n",
    "\n",
    "docs2bow_train, docs2bow_heldout = list(), list()\n",
    "for doc2bow in docs2bow:\n",
    "    doc2bow_train, doc2bow_heldout = list(), list()\n",
    "    for w, ndw in doc2bow:\n",
    "        ndw_train = randint(ndw+1)\n",
    "        if ndw_train > 0:\n",
    "            doc2bow_train += [(w, ndw_train)]\n",
    "        if ndw - ndw_train > 0:\n",
    "            doc2bow_heldout += [(w, ndw - ndw_train)]\n",
    "    docs2bow_train += [(doc2bow_train)]\n",
    "    docs2bow_heldout += [(doc2bow_heldout)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "metadata": {},
     "execution_count": 70
    }
   ],
   "source": [
    "len(docs2bow_heldout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[(0, 1)],\n",
       " [(4, 1), (5, 1), (6, 1)],\n",
       " [(8, 1), (4, 1), (5, 1)],\n",
       " [(5, 1), (0, 1), (8, 1)],\n",
       " [(4, 1), (6, 1), (7, 1)],\n",
       " [],\n",
       " [],\n",
       " [(10, 1), (11, 1)],\n",
       " [(11, 1)]]"
      ]
     },
     "metadata": {},
     "execution_count": 71
    }
   ],
   "source": [
    "docs2bow_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "metadata": {},
     "execution_count": 72
    }
   ],
   "source": [
    "len(docs2bow_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[(1, 1), (2, 1)],\n",
       " [(3, 1), (2, 1), (7, 1)],\n",
       " [(1, 1)],\n",
       " [(5, 1)],\n",
       " [],\n",
       " [(9, 1)],\n",
       " [(10, 1), (9, 1)],\n",
       " [(9, 1)],\n",
       " [(10, 1), (3, 1)]]"
      ]
     },
     "metadata": {},
     "execution_count": 73
    }
   ],
   "source": [
    "docs2bow_heldout"
   ]
  },
  {
   "source": [
    "## TEM for train and heldout"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify likelihood to solve the problem of omitted words or docs\n",
    "# in splitting, may some words be omitted entirely from training corpus\n",
    "# therefore their condit probs P(w|z) remain zero in training procedure\n",
    "# why? note that EM-steps works only with training corpus and obviously omitted words left unseen in training\n",
    "# and also note that repars accumulate from zero for \"seen\" data, so repars for unseen data remain zero through M-step\n",
    "# so in evaluating performance on held-out data by likelihood(pars, heldout), Pcocur(omitted) would be 0 (P(w|z) = 0)\n",
    "# and it diverges log-likelihood!\n",
    "# so for avoiding this problem, we ignore omitted words in log-likelihood calculations\n",
    "# similar problem could happen for omitted docs\n",
    "def likelihood(pars, docs2bow):\n",
    "    Pz, Pd_z, Pw_z = pars\n",
    "    L = 0\n",
    "    for d, doc2bow in enumerate(docs2bow):\n",
    "        for w, ndw in doc2bow:\n",
    "            Pcocur = sum(Pz[:] * Pd_z[d,:] * Pw_z[w, :])\n",
    "            # modification\n",
    "            if Pcocur == 0: continue\n",
    "            L += ndw * np.log(Pcocur)\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEM-steps on training corpus and evaluating performance by likelihood on held-out corpus\n",
    "def TEMsteps(beta_runtimes, first_beta, eta, docs2bow_train, docs2bow_heldout):\n",
    "    beta = first_beta\n",
    "    pars = random_init_pars(2, n.shape)\n",
    "    for i in range(beta_runtimes):\n",
    "        print(beta)\n",
    "        # pars = random_init_pars(2, n.shape)\n",
    "        # print(pars)\n",
    "        new_likeli = likelihood(pars, docs2bow_heldout) # first likelihood in new beta\n",
    "        likeli = -np.inf    # for assuring the entrance to while loop\n",
    "        while round(new_likeli, 2) > round(likeli, 2):  # check if likelihood increased?\n",
    "            likeli = new_likeli\n",
    "            print(likeli)   # print increased likelihood (or first likelihood)\n",
    "            # one TEM step\n",
    "            prepars = pars  # save pars before executing TEM, for undoing pars if khodayi nakarde likelihood decreased\n",
    "            posters = TEstep(beta, pars, docs2bow_train)    # TE-step\n",
    "            pars = TMstep(posters, docs2bow_train)  # TM-step\n",
    "            # print(pars)\n",
    "            new_likeli = likelihood(pars, docs2bow_heldout) # calculating likelihood for re-estimated pars\n",
    "        print(new_likeli)   # print decreased (inappropriate) likelihood\n",
    "        pars = prepars  # undo pars\n",
    "        beta *= eta # new beta\n",
    "    return pars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.0\n-62.58641863030034\n-13.607437551924104\n-12.87357338851632\n-12.384199296951717\n-12.301318690268594\n-12.325084320654966\n0.9\n-12.301318690268594\n-12.304514953883997\n0.81\n-12.301318690268594\n-12.283137942255992\n-12.27477053867144\n-12.265384659687221\n0.7290000000000001\n-12.27477053867144\n-12.249740186530769\n-12.237182458236951\n-12.230039870027895\n-12.225327302449305\n0.6561000000000001\n-12.230039870027895\n-12.211898648551669\n-12.203014415163382\n-12.198538405417658\n0.5904900000000002\n-12.203014415163382\n-12.182552276137564\n-12.1708069042555\n-12.16377066394478\n-12.15950413129255\n0.5314410000000002\n-12.16377066394478\n-12.141853327769564\n-12.13064594502435\n-12.125791904508848\n0.47829690000000014\n-12.13064594502435\n-12.11658550906781\n-12.11952830836163\n0.43046721000000016\n-12.11658550906781\n-12.122139583605126\n0.38742048900000015\n-12.11658550906781\n-12.129252287614332\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(array([0.57903684, 0.42096316]),\n",
       " array([[1.80351313e-03, 1.45988306e-01],\n",
       "        [3.08234273e-01, 2.14294332e-02],\n",
       "        [2.52841947e-01, 9.76218410e-02],\n",
       "        [1.13525923e-01, 2.89251695e-01],\n",
       "        [3.23594344e-01, 3.01582692e-04],\n",
       "        [0.00000000e+00, 0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00],\n",
       "        [5.96579468e-67, 2.96938095e-01],\n",
       "        [8.78508855e-68, 1.48469047e-01]]),\n",
       " array([[1.59886880e-02, 2.74945578e-01],\n",
       "        [0.00000000e+00, 0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00],\n",
       "        [3.12837638e-01, 1.50974834e-02],\n",
       "        [2.32391491e-01, 1.25751541e-01],\n",
       "        [2.15209784e-01, 9.16013662e-04],\n",
       "        [1.07937655e-01, 2.89916041e-07],\n",
       "        [1.15634746e-01, 1.37881952e-01],\n",
       "        [0.00000000e+00, 0.00000000e+00],\n",
       "        [3.86541675e-67, 1.48469047e-01],\n",
       "        [2.97888678e-67, 2.96938095e-01]]))"
      ]
     },
     "metadata": {},
     "execution_count": 78
    }
   ],
   "source": [
    "# to find the approp beta, try TEM for various betas, on training and held-out corpus\n",
    "TEMsteps(10, 1.00, 0.90, docs2bow_train, docs2bow_heldout)"
   ]
  },
  {
   "source": [
    "## Final TEM by appropriate beta"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.53\n-144.85798331262936\n-131.7642127029726\n-131.72140846998877\n-131.8479417952931\n"
     ]
    }
   ],
   "source": [
    "# to obtain the learned pars, one TEM by approp beta, on the whole corpus\n",
    "tem_learned_pars = TEMsteps(1, 0.53, 0.00, docs2bow, docs2bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.0\n-134.95083987283653\n-129.03858484983505\n-126.8821700637002\n-124.79179094753185\n-122.6024058997259\n-120.55752118757084\n-119.30317783424297\n-118.6376319462297\n-118.23929456252168\n-117.99123080907067\n-117.86316109375669\n-117.8129033307111\n-117.79664605470663\n-117.79180365670419\n-117.79035348392607\n"
     ]
    }
   ],
   "source": [
    "# one EM (TEM by beta = 1), on the whole corpus\n",
    "em_learned_pars = TEMsteps(1, 1.00, 0.00, docs2bow, docs2bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just for curiosity, let's calculate likelihood by ndw * log(ndw/N)\n",
    "def likelihood(pars, docs2bow):\n",
    "    Pz, Pd_z, Pw_z = pars\n",
    "    L = 0\n",
    "    N = sum(idf)    # N: total number of co-occurs, means total number of words seen in documents\n",
    "    for d, doc2bow in enumerate(docs2bow):\n",
    "        for w, ndw in doc2bow:\n",
    "            # Pcocur = sum(Pz[:] * Pd_z[d,:] * Pw_z[w, :])\n",
    "            # if Pcocur == 0: continue\n",
    "            # L += ndw * np.log(Pcocur)\n",
    "            L += ndw * np.log(ndw/N)\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "-120.11369315764638"
      ]
     },
     "metadata": {},
     "execution_count": 100
    }
   ],
   "source": [
    "pars = 0, 0, 0\n",
    "likelihood(pars, docs2bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return likelihood to its original definition\n",
    "def likelihood(pars, docs2bow):\n",
    "    Pz, Pd_z, Pw_z = pars\n",
    "    L = 0\n",
    "    for d, doc2bow in enumerate(docs2bow):\n",
    "        for w, ndw in doc2bow:\n",
    "            Pcocur = sum(Pz[:] * Pd_z[d,:] * Pw_z[w, :])\n",
    "            # modification\n",
    "            if Pcocur == 0: continue\n",
    "            L += ndw * np.log(Pcocur)\n",
    "    return L"
   ]
  },
  {
   "source": [
    "## Looking at parameters"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2token = {tokenid: token for token, tokenid in token2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{0: 'human',\n",
       " 1: 'interface',\n",
       " 2: 'computer',\n",
       " 3: 'survey',\n",
       " 4: 'user',\n",
       " 5: 'system',\n",
       " 6: 'response',\n",
       " 7: 'time',\n",
       " 8: 'eps',\n",
       " 9: 'trees',\n",
       " 10: 'graph',\n",
       " 11: 'minors'}"
      ]
     },
     "metadata": {},
     "execution_count": 108
    }
   ],
   "source": [
    "id2token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token2Pw_z(z, pars, first_probs=None):\n",
    "    Pz, Pd_z, Pw_z = pars\n",
    "    token2Pw_z = {id2token[tokenid]: prob for tokenid, prob in enumerate(Pw_z[:, z])}\n",
    "    sorted_token2Pw_z = dict(sorted(token2Pw_z.items(), key=lambda item: -item[1])[:first_probs])\n",
    "    return sorted_token2Pw_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'system': 0.29873592517649095,\n",
       " 'eps': 0.1493680871428115,\n",
       " 'human': 0.14936808714281138,\n",
       " 'interface': 0.14936808714281138,\n",
       " 'computer': 0.14930285564027432,\n",
       " 'user': 0.10376415419227746,\n",
       " 'survey': 8.771550379004421e-05,\n",
       " 'response': 3.5885623225757856e-06,\n",
       " 'time': 1.4994964104204717e-06,\n",
       " 'minors': 1.2154897487940247e-40,\n",
       " 'graph': 5.0783526089691957e-42,\n",
       " 'trees': 3.0325620284683865e-139}"
      ]
     },
     "metadata": {},
     "execution_count": 114
    }
   ],
   "source": [
    "token2Pw_z(0, em_learned_pars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'graph': 0.192181308416908,\n",
       " 'trees': 0.19218130841690736,\n",
       " 'minors': 0.12812087227793845,\n",
       " 'time': 0.12811958608092178,\n",
       " 'response': 0.12811779417910374,\n",
       " 'survey': 0.12804563407228933,\n",
       " 'user': 0.10317733038787714,\n",
       " 'computer': 5.595249403593857e-05,\n",
       " 'system': 2.136740181661542e-07,\n",
       " 'eps': 2.1096050166392114e-19,\n",
       " 'interface': 2.8476247908790796e-20,\n",
       " 'human': 1.5438746449157202e-150}"
      ]
     },
     "metadata": {},
     "execution_count": 115
    }
   ],
   "source": [
    "token2Pw_z(1, em_learned_pars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}