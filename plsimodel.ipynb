{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoplist = set('for a of the and to rt'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoplist = set('for a of the and to rt is are in on from an'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'an',\n",
       " 'and',\n",
       " 'are',\n",
       " 'for',\n",
       " 'from',\n",
       " 'in',\n",
       " 'is',\n",
       " 'of',\n",
       " 'on',\n",
       " 'rt',\n",
       " 'the',\n",
       " 'to'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stoplist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A small corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'Human machine interface for Lab ABC computer applications',\n",
    "    'A survey of user opinion of computer system response time',\n",
    "    'The EPS user interface management system',\n",
    "    'System and human system engineering testing of EPS',\n",
    "    'Relation of user-perceived response time to error measurement',\n",
    "    'The generation of random, binary, unordered trees',\n",
    "    'The intersection graph of paths in trees',\n",
    "    'Graph minors IV: Width of trees and well-quasi-ordering',\n",
    "    'Graph minors: A survey'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('deerwester.txt', 'wb') as f:\n",
    "    pickle.dump(corpus, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating generator object for streaming tweets\n",
    "class Tweets:\n",
    "    def __iter__(self):\n",
    "        for tweet in pickle.load(open('deerwester.txt', 'rb')):\n",
    "            yield tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1)]\n",
      "1 [(7, 1), (8, 1), (9, 1), (5, 1), (10, 1), (11, 1), (12, 1)]\n",
      "2 [(13, 1), (8, 1), (2, 1), (14, 1), (10, 1)]\n",
      "3 [(10, 2), (0, 1), (15, 1), (16, 1), (13, 1)]\n",
      "4 [(17, 1), (8, 1), (18, 1), (11, 1), (12, 1), (19, 1), (20, 1)]\n",
      "5 [(21, 1), (22, 1), (23, 1), (24, 1), (25, 1)]\n",
      "6 [(26, 1), (27, 1), (28, 1), (29, 1), (25, 1)]\n",
      "7 [(27, 1), (30, 1), (31, 1), (32, 1), (25, 1), (33, 1), (34, 1), (35, 1)]\n",
      "8 [(27, 1), (30, 1), (7, 1)]\n"
     ]
    }
   ],
   "source": [
    "# streaming corpus and storing documents in bow representation\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "tweets = Tweets()\n",
    "token2id = {}\n",
    "# token2id : dict of (token(str), tokenId(int))\n",
    "idf = defaultdict(int)\n",
    "# idf: dict of (tokenId, freq = frequency of tokenId in corpus)\n",
    "docs2bow = []\n",
    "# docs2bow: list of [doc2bow]\n",
    "# doc2bow: list of (tokenIds in doc, docfreq = frequency of tokenId in doc)\n",
    "for docno, tweet in enumerate(tweets):\n",
    "    # lowering tweets and removing punctuations from it, then splitting\n",
    "    document = re.sub(r'[-–,:;|.!?*()+&/~<>=\"]', ' ', tweet.lower()).split()\n",
    "    counter = defaultdict(int)\n",
    "    # counter: dict of (tokenIds in doc, docfreq = frequency of tokenId in doc)\n",
    "    for word in document:\n",
    "        if word in stoplist: continue   # check word by stoplist\n",
    "        if word not in token2id: token2id[word] = len(token2id) # add word as a token if seen for the first time\n",
    "        counter[word] += 1\n",
    "        idf[token2id[word]] += 1\n",
    "    # creating doc2bow for this doc\n",
    "    doc2bow = [(token2id[word], docfreq) for word, docfreq in counter.items()]\n",
    "    print(docno, doc2bow)\n",
    "    # append doc2bow to docs2bow\n",
    "    docs2bow.append(doc2bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A user timeline corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy as tw\n",
    "\n",
    "# load keys\n",
    "with open('keys.txt', 'rb') as f:\n",
    "    keys = pickle.load(f)\n",
    "# define keys\n",
    "consumer_key = keys['consumer_key']\n",
    "consumer_secret = keys['consumer_secret']\n",
    "access_token = keys['access_token']\n",
    "access_token_secret = keys['access_token_secret']\n",
    "# authenticate and create api object\n",
    "auth = tw.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tw.API(auth, wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator object for iterating through user timeline\n",
    "class Tweets():\n",
    "    def __init__(self, pagination_num=3):\n",
    "        self.pagination_num = pagination_num\n",
    "        self.cursor = tw.Cursor(api.user_timeline, id=\"unicef\", # id = \"indykaila\"\n",
    "                              exclude_replies=True,\n",
    "                              include_rts=True,\n",
    "                              tweet_mode='extended').pages(self.pagination_num)\n",
    "    def __iter__(self):\n",
    "        for page in self.cursor:\n",
    "            for status in page:\n",
    "                yield status.full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# streaming corpus and storing documents in bow representation\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from IPython.display import clear_output\n",
    "\n",
    "tweets = Tweets(100)\n",
    "token2id = {}\n",
    "idf = defaultdict(int)\n",
    "docs2bow = []\n",
    "for docno, tweet in enumerate(tweets):\n",
    "    # remove links from tweets\n",
    "    tweet = re.sub(r'\\bhttps:\\S+', '', tweet.lower())\n",
    "    # print(tweet)\n",
    "    document = re.sub(r'[-–,:;|.!?*()+&/~<>=\"]', ' ', tweet).split()\n",
    "    # print(document)\n",
    "    counter = defaultdict(int)\n",
    "    for word in document:\n",
    "        if word in stoplist: continue\n",
    "        if word not in token2id: token2id[word] = len(token2id)\n",
    "        counter[word] += 1\n",
    "        idf[token2id[word]] += 1\n",
    "    doc2bow = [(token2id[word], docfreq) for word, docfreq in counter.items()]\n",
    "    # print(docno, doc2bow)\n",
    "    print(docno)\n",
    "    docs2bow.append(doc2bow)\n",
    "clear_output()\n",
    "print(docno)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Health news dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoplist = set('for a of the and to rt is are in on from an video'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HealthTweets():\n",
    "    def __iter__(self):\n",
    "        for line in open('bbchealth.txt', 'r').readlines():\n",
    "            text = line.split('|')[2]\n",
    "            yield text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3928\n"
     ]
    }
   ],
   "source": [
    "# streaming corpus and storing documents in bow representation\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from IPython.display import clear_output\n",
    "\n",
    "tweets = HealthTweets()\n",
    "token2id = {}\n",
    "idf = defaultdict(int)\n",
    "docs2bow = []\n",
    "for docno, tweet in enumerate(tweets):\n",
    "    # remove links from tweets\n",
    "    tweet = re.sub(r'\\bhttp:\\S+', '', tweet.lower())\n",
    "    # print(tweet)\n",
    "    document = re.sub(r\"[-–,:;|.!?*()+&/~<>=']\", ' ', tweet).split()\n",
    "    # print(document)\n",
    "    counter = defaultdict(int)\n",
    "    for word in document:\n",
    "        if word in stoplist: continue\n",
    "        if word not in token2id: token2id[word] = len(token2id)\n",
    "        counter[word] += 1\n",
    "        idf[token2id[word]] += 1\n",
    "    doc2bow = [(token2id[word], docfreq) for word, docfreq in counter.items()]\n",
    "    # print(docno, doc2bow)\n",
    "    print(docno)\n",
    "    docs2bow.append(doc2bow)\n",
    "clear_output()\n",
    "print(docno)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4509"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'human': 0,\n",
       " 'machine': 1,\n",
       " 'interface': 2,\n",
       " 'lab': 3,\n",
       " 'abc': 4,\n",
       " 'computer': 5,\n",
       " 'applications': 6,\n",
       " 'survey': 7,\n",
       " 'user': 8,\n",
       " 'opinion': 9,\n",
       " 'system': 10,\n",
       " 'response': 11,\n",
       " 'time': 12,\n",
       " 'eps': 13,\n",
       " 'management': 14,\n",
       " 'engineering': 15,\n",
       " 'testing': 16,\n",
       " 'relation': 17,\n",
       " 'perceived': 18,\n",
       " 'error': 19,\n",
       " 'measurement': 20,\n",
       " 'generation': 21,\n",
       " 'random': 22,\n",
       " 'binary': 23,\n",
       " 'unordered': 24,\n",
       " 'trees': 25,\n",
       " 'intersection': 26,\n",
       " 'graph': 27,\n",
       " 'paths': 28,\n",
       " 'in': 29,\n",
       " 'minors': 30,\n",
       " 'iv': 31,\n",
       " 'width': 32,\n",
       " 'well': 33,\n",
       " 'quasi': 34,\n",
       " 'ordering': 35}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4509"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {0: 2,\n",
       "             1: 1,\n",
       "             2: 2,\n",
       "             3: 1,\n",
       "             4: 1,\n",
       "             5: 2,\n",
       "             6: 1,\n",
       "             7: 2,\n",
       "             8: 3,\n",
       "             9: 1,\n",
       "             10: 4,\n",
       "             11: 2,\n",
       "             12: 2,\n",
       "             13: 2,\n",
       "             14: 1,\n",
       "             15: 1,\n",
       "             16: 1,\n",
       "             17: 1,\n",
       "             18: 1,\n",
       "             19: 1,\n",
       "             20: 1,\n",
       "             21: 1,\n",
       "             22: 1,\n",
       "             23: 1,\n",
       "             24: 1,\n",
       "             25: 3,\n",
       "             26: 1,\n",
       "             27: 3,\n",
       "             28: 1,\n",
       "             29: 1,\n",
       "             30: 2,\n",
       "             31: 1,\n",
       "             32: 1,\n",
       "             33: 1,\n",
       "             34: 1,\n",
       "             35: 1})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3929"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs2bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1)],\n",
       " [(7, 1), (8, 1), (9, 1), (5, 1), (10, 1), (11, 1), (12, 1)],\n",
       " [(13, 1), (8, 1), (2, 1), (14, 1), (10, 1)],\n",
       " [(10, 2), (0, 1), (15, 1), (16, 1), (13, 1)],\n",
       " [(17, 1), (8, 1), (18, 1), (11, 1), (12, 1), (19, 1), (20, 1)],\n",
       " [(21, 1), (22, 1), (23, 1), (24, 1), (25, 1)],\n",
       " [(26, 1), (27, 1), (28, 1), (29, 1), (25, 1)],\n",
       " [(27, 1), (30, 1), (31, 1), (32, 1), (25, 1), (33, 1), (34, 1), (35, 1)],\n",
       " [(27, 1), (30, 1), (7, 1)]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs2bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter once words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter once words\n",
    "bad_ids = set(tokenid for tokenid, freq in idf.items() if freq == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2301"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bad_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1,\n",
       " 3,\n",
       " 4,\n",
       " 6,\n",
       " 9,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 26,\n",
       " 28,\n",
       " 29,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update token2id and idf which filtered once words\n",
    "token2id = {token: tokenid for token, tokenid in token2id.items() if idf[tokenid] > 1}\n",
    "idf = {tokenid: freq for tokenid, freq in idf.items() if freq > 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2208"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'human': 0,\n",
       " 'interface': 2,\n",
       " 'computer': 5,\n",
       " 'survey': 7,\n",
       " 'user': 8,\n",
       " 'system': 10,\n",
       " 'response': 11,\n",
       " 'time': 12,\n",
       " 'eps': 13,\n",
       " 'trees': 25,\n",
       " 'graph': 27,\n",
       " 'minors': 30}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 2, 2: 2, 5: 2, 7: 2, 8: 3, 10: 4, 11: 2, 12: 2, 13: 2, 25: 3, 27: 3, 30: 2}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idmap: maps old tokenIds to new ordered tokenIds\n",
    "idmap = dict(zip(sorted(token2id.values()), range(len(token2id))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2208"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(idmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0,\n",
       " 2: 1,\n",
       " 5: 2,\n",
       " 7: 3,\n",
       " 8: 4,\n",
       " 10: 5,\n",
       " 11: 6,\n",
       " 12: 7,\n",
       " 13: 8,\n",
       " 25: 9,\n",
       " 27: 10,\n",
       " 30: 11}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note this cell is one time run\n",
    "# compactify token2id and idf\n",
    "token2id = {token: idmap[tokenid] for token, tokenid in token2id.items()}\n",
    "idf = {idmap[tokenid]: freq for tokenid, freq in idf.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'human': 0,\n",
       " 'interface': 1,\n",
       " 'computer': 2,\n",
       " 'survey': 3,\n",
       " 'user': 4,\n",
       " 'system': 5,\n",
       " 'response': 6,\n",
       " 'time': 7,\n",
       " 'eps': 8,\n",
       " 'trees': 9,\n",
       " 'graph': 10,\n",
       " 'minors': 11}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 2, 1: 2, 2: 2, 3: 2, 4: 3, 5: 4, 6: 2, 7: 2, 8: 2, 9: 3, 10: 3, 11: 2}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1)],\n",
       " [(7, 1), (8, 1), (9, 1), (5, 1), (10, 1), (11, 1), (12, 1)],\n",
       " [(13, 1), (8, 1), (2, 1), (14, 1), (10, 1)],\n",
       " [(10, 2), (0, 1), (15, 1), (16, 1), (13, 1)],\n",
       " [(17, 1), (8, 1), (18, 1), (11, 1), (12, 1), (19, 1), (20, 1)],\n",
       " [(21, 1), (22, 1), (23, 1), (24, 1), (25, 1)],\n",
       " [(26, 1), (27, 1), (28, 1), (29, 1), (25, 1)],\n",
       " [(27, 1), (30, 1), (31, 1), (32, 1), (25, 1), (33, 1), (34, 1), (35, 1)],\n",
       " [(27, 1), (30, 1), (7, 1)]]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# token2id changed, but docs2bow still has the same old tokenIds\n",
    "docs2bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rebuild docs2bow based on new token2id\n",
    "docs2bow = [\n",
    "    [(idmap[tokenid], docfreq) for tokenid, docfreq in doc2bow if tokenid not in bad_ids]\n",
    "    for doc2bow in docs2bow\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save/load the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1)],\n",
       " [(3, 1), (4, 1), (2, 1), (5, 1), (6, 1), (7, 1)],\n",
       " [(8, 1), (4, 1), (1, 1), (5, 1)],\n",
       " [(5, 2), (0, 1), (8, 1)],\n",
       " [(4, 1), (6, 1), (7, 1)],\n",
       " [(9, 1)],\n",
       " [(10, 1), (9, 1)],\n",
       " [(10, 1), (11, 1), (9, 1)],\n",
       " [(10, 1), (11, 1), (3, 1)]]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs2bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `docs2bow`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the corpus in bow representation\n",
    "with open('unicef_corpus2bow.txt', 'wb') as f:\n",
    "    pickle.dump(docs2bow, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the corpus in bow representation\n",
    "with open('unicef_corpus2bow.txt', 'rb') as f:\n",
    "    docs2bow = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `token2id`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save token2id\n",
    "with open('unicef_token2id.txt', 'wb') as f:\n",
    "    pickle.dump(token2id, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load token2id\n",
    "with open('unicef_token2id.txt', 'rb') as f:\n",
    "    token2id = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# not necessary, just n.shape needed for further uses\n",
    "# note that we don't use n(d,w) matrix in computations, n(d,w) is presented in docs2bow as ndw that you'll see\n",
    "# creating words2bod by docs2bow -> n -> n.T -> words2bod\n",
    "# words2bod shows each word appeared in which docs\n",
    "import numpy as np\n",
    "# n: numpy array of n[d][w] = n(d,w)\n",
    "# d = document number, w = word's tokenId\n",
    "n = np.zeros((len(docs2bow), len(token2id)))\n",
    "for docno, doc2bow in enumerate(docs2bow):\n",
    "    for tokenid, docfreq in doc2bow:\n",
    "        n[docno, tokenid] += docfreq\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3929, 2208)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 1., 1., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 1., 1., 2., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 1., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 1., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 1.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "words2bod = [\n",
    "    [(tokenid, int(docfreq)) for tokenid, docfreq in enumerate(rows) if docfreq != 0]\n",
    "    for rows in n.T\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (3, 1)],\n",
       " [(0, 1), (2, 1)],\n",
       " [(0, 1), (1, 1)],\n",
       " [(1, 1), (8, 1)],\n",
       " [(1, 1), (2, 1), (4, 1)],\n",
       " [(1, 1), (2, 1), (3, 2)],\n",
       " [(1, 1), (4, 1)],\n",
       " [(1, 1), (4, 1)],\n",
       " [(2, 1), (3, 1)],\n",
       " [(5, 1), (6, 1), (7, 1)],\n",
       " [(6, 1), (7, 1), (8, 1)],\n",
       " [(7, 1), (8, 1)]]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words2bod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLSI model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters and likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 20   # K: number of topics considered, namely the size of latent semantic set Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import rand\n",
    "\n",
    "def random_init_pars(K, nshape):\n",
    "    N, M = nshape   # N = number of documents, M = number of tokens\n",
    "    Pz = rand(K); Pz /= sum(Pz) # P(z)\n",
    "    Pd_z = rand(N, K); Pd_z /= Pd_z.sum(axis=0) # P(d|z)\n",
    "    Pw_z = rand(M, K); Pw_z /= Pw_z.sum(axis=0) # P(w|z)\n",
    "    pars = Pz, Pd_z, Pw_z   # pack parameters in a variable called pars\n",
    "    return pars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1440, 2481)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.4001385, 0.5998615]),\n",
       " array([[0.11300014, 0.13423582],\n",
       "        [0.15208975, 0.14145356],\n",
       "        [0.11296352, 0.14415165],\n",
       "        [0.10676998, 0.05131162],\n",
       "        [0.14655433, 0.0403815 ],\n",
       "        [0.02229351, 0.17183628],\n",
       "        [0.11544259, 0.166549  ],\n",
       "        [0.14183071, 0.10509881],\n",
       "        [0.08905547, 0.04498175]]),\n",
       " array([[0.01186479, 0.08050633],\n",
       "        [0.12389336, 0.03654783],\n",
       "        [0.03002695, 0.18772072],\n",
       "        [0.08509484, 0.13924188],\n",
       "        [0.17923298, 0.00066867],\n",
       "        [0.12707695, 0.13626144],\n",
       "        [0.17833826, 0.14977082],\n",
       "        [0.0883424 , 0.14072519],\n",
       "        [0.03751731, 0.06700934],\n",
       "        [0.09520133, 0.02886329],\n",
       "        [0.0124025 , 0.02764159],\n",
       "        [0.03100834, 0.0050429 ]]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_init_pars(K, n.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood(pars, docs2bow):\n",
    "    Pz, Pd_z, Pw_z = pars   # unpack parameters\n",
    "    L = 0\n",
    "    # iterate over data in docs2bow and calculate prob of co-occur for them, based on pars\n",
    "    for d, doc2bow in enumerate(docs2bow):\n",
    "        for w, ndw in doc2bow:\n",
    "            Pcocur = sum(Pz[:] * Pd_z[d,:] * Pw_z[w, :])    # P(d,w)\n",
    "            # adding up all log-likelihood terms\n",
    "            L += ndw * np.log(Pcocur)\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-284886.5492200772"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "likelihood(random_init_pars(K, n.shape), docs2bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expectation step\n",
    "def Estep(pars, docs2bow):  # no necessity to pass docs2bow (data) to Estep, but it'll help to decrease computations\n",
    "    Pz, Pd_z, Pw_z = pars\n",
    "    posters = np.zeros((len(Pz), len(Pd_z), len(Pw_z)))\n",
    "    # posters could be an attribute and no need to reset to zeros because it's not accumulative\n",
    "    # iterate through data and calculate posteriors just for seen pairs of (d, w)\n",
    "    # so unseen posteriors left to be zero\n",
    "    for z in range(len(Pz)):\n",
    "        for d, doc2bow in enumerate(docs2bow):\n",
    "            for w, ndw in doc2bow:\n",
    "                posters[z, d, w] = Pz[z] * Pd_z[d, z] * Pw_z[w, z]\n",
    "    # normalization\n",
    "    posters /= posters.sum(axis=0) + 1e-16  # a tiny number added just to avoid dividing by zero error for unseen (d, w)s\n",
    "    return posters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.32303151, 0.54699039, 0.49943317, ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.45525299, 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.02473211, 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        ...,\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.48931705]],\n",
       "\n",
       "       [[0.14450733, 0.11202042, 0.21154905, ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.18117445, 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.54418821, 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        ...,\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.37251504]],\n",
       "\n",
       "       [[0.03987279, 0.06395888, 0.04780066, ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.05434521, 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.04146302, 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        ...,\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.09508453]],\n",
       "\n",
       "       [[0.42527003, 0.27029655, 0.22179339, ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.0383428 , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.11869039, 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        ...,\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.03928056]],\n",
       "\n",
       "       [[0.06731834, 0.00673376, 0.01942374, ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.27088455, 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.27092626, 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        ...,\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.00380282]]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posters = Estep(random_init_pars(K, n.shape), docs2bow)\n",
    "posters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 1440, 2481)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posters.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# is posters normalized?\n",
    "posters.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximization step\n",
    "def Mstep(posters, docs2bow):\n",
    "    # re-estimation of the parameters by posteriors calculated in E-step based on parameters\n",
    "    K, N, M = posters.shape  # K, N, M could be in an attribute self.archit\n",
    "    rePz, rePd_z, rePw_z = np.zeros(K), np.zeros((N, K)), np.zeros((M, K))\n",
    "    # repars should reset to zeros in each M-step because they'd be calculated accumulatively\n",
    "    # iterate over data and add up terms n(d,w) * poster(z|d,w) to associated repars\n",
    "    for z in range(K):\n",
    "        for d, doc2bow in enumerate(docs2bow):\n",
    "            for w, ndw in doc2bow:\n",
    "                rePz[z] += ndw * posters[z, d, w]\n",
    "                rePd_z[d, z] += ndw * posters[z, d, w]\n",
    "                rePw_z[w, z] += ndw * posters[z, d, w]\n",
    "    # normalization\n",
    "    rePz /= sum(rePz)\n",
    "    rePd_z /= rePd_z.sum(axis=0)\n",
    "    rePw_z /= rePw_z.sum(axis=0)\n",
    "    repars = rePz, rePd_z, rePw_z   # pack re-estimated parameters in repars and return it\n",
    "    return repars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0.11050252, 0.49422261, 0.04013839, 0.02990187, 0.32523461]), array([[1.77758908e-05, 1.36297978e-03, 1.85012743e-04, 2.80483553e-04,\n",
      "        6.69807474e-04],\n",
      "       [8.46433053e-04, 7.95912790e-04, 6.46290209e-05, 4.49213506e-04,\n",
      "        5.32544229e-04],\n",
      "       [3.08975295e-04, 5.33615596e-05, 2.38025249e-05, 4.60703674e-04,\n",
      "        6.08415677e-04],\n",
      "       ...,\n",
      "       [1.23670783e-03, 9.65226472e-04, 9.69355921e-04, 1.97738257e-04,\n",
      "        1.35466950e-03],\n",
      "       [4.34613088e-04, 1.19494630e-03, 2.15061623e-04, 3.16974529e-04,\n",
      "        1.02119677e-03],\n",
      "       [1.38109975e-03, 8.80540174e-04, 1.01815017e-04, 4.32503093e-06,\n",
      "        9.90454460e-04]]), array([[4.37660960e-04, 6.82640865e-05, 7.63038848e-04, 6.16164874e-04,\n",
      "        7.03154309e-04],\n",
      "       [6.47898319e-04, 2.00939033e-04, 9.62942829e-05, 5.06505313e-04,\n",
      "        2.64907051e-04],\n",
      "       [8.03226433e-04, 6.02588360e-04, 1.15622529e-04, 1.96341919e-04,\n",
      "        4.76100360e-04],\n",
      "       ...,\n",
      "       [3.58202328e-04, 5.13095516e-04, 2.27278458e-04, 1.03024729e-04,\n",
      "        3.79465363e-04],\n",
      "       [1.75420297e-05, 2.10534280e-04, 6.62090450e-05, 1.46433182e-04,\n",
      "        1.07948681e-04],\n",
      "       [3.32803996e-04, 2.13531422e-04, 5.16789637e-04, 4.71200821e-04,\n",
      "        4.37746625e-04]]))\n",
      "-465275.858646231\n",
      "(array([0.14424754, 0.4372343 , 0.05386372, 0.03947093, 0.32518351]), array([[1.00345112e-05, 6.10317012e-04, 8.96731025e-05, 1.50558355e-04,\n",
      "        4.50906441e-04],\n",
      "       [1.75576196e-03, 1.41038797e-03, 1.26820086e-04, 8.83999461e-04,\n",
      "        1.22449581e-03],\n",
      "       [7.78625348e-04, 8.71225657e-05, 4.40495520e-05, 7.08911456e-04,\n",
      "        1.05532889e-03],\n",
      "       ...,\n",
      "       [4.50151609e-04, 4.24876062e-04, 3.04299628e-04, 7.94440213e-05,\n",
      "        4.78096319e-04],\n",
      "       [1.67019192e-04, 4.39942969e-04, 6.80845272e-05, 1.38724112e-04,\n",
      "        4.13962675e-04],\n",
      "       [1.57185633e-03, 1.03731687e-03, 1.13314439e-04, 5.07840053e-06,\n",
      "        1.01031499e-03]]), array([[5.42244650e-03, 1.53544406e-03, 1.00919913e-02, 9.09431182e-03,\n",
      "        9.26947907e-03],\n",
      "       [2.67935111e-04, 1.02599711e-04, 2.97664093e-05, 2.08923188e-04,\n",
      "        2.16405861e-04],\n",
      "       [2.02720006e-04, 2.47931471e-04, 3.64603056e-05, 4.73012135e-05,\n",
      "        2.69834381e-04],\n",
      "       ...,\n",
      "       [1.22514380e-05, 1.51759917e-04, 2.59834975e-05, 8.69975611e-06,\n",
      "        8.72535778e-05],\n",
      "       [1.56008892e-05, 8.01120630e-05, 6.91376538e-05, 1.09944347e-04,\n",
      "        6.19663197e-05],\n",
      "       [8.03777140e-05, 4.85151844e-05, 8.95050645e-06, 3.52431388e-07,\n",
      "        9.89883057e-05]]))\n",
      "-422221.2307713599\n"
     ]
    }
   ],
   "source": [
    "# just one EM-step\n",
    "pars = random_init_pars(K, n.shape)\n",
    "print(pars)\n",
    "print(likelihood(pars, docs2bow))\n",
    "# EM\n",
    "posters = Estep(pars, docs2bow)\n",
    "repars = Mstep(posters, docs2bow)\n",
    "print(repars)\n",
    "print(likelihood(repars, docs2bow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "[1. 1. 1. 1. 1.]\n",
      "[1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# check whether parameters remain normalized\n",
    "Pz, Pd_z, Pw_z = repars\n",
    "print(Pz.sum(axis=0))\n",
    "print(Pd_z.sum(axis=0))\n",
    "print(Pw_z.sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expectation Maximization steps\n",
    "def EMsteps(runtimes, pars, docs2bow):\n",
    "    print(pars)\n",
    "    print(likelihood(pars, docs2bow))\n",
    "    for runtime in range(runtimes):\n",
    "        posters = Estep(pars, docs2bow)\n",
    "        pars = Mstep(posters, docs2bow)\n",
    "        print(likelihood(pars, docs2bow))\n",
    "    print(pars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0.17723933, 0.2468415 , 0.27925253, 0.24298058, 0.05368606]), array([[1.23898491e-03, 4.07932364e-04, 7.72285110e-04, 4.91432597e-04,\n",
      "        5.77671454e-04],\n",
      "       [1.00782306e-04, 7.95301050e-04, 8.53073129e-04, 1.27229277e-03,\n",
      "        7.45132328e-04],\n",
      "       [1.04888451e-03, 4.67003607e-04, 2.48203817e-05, 7.77393756e-04,\n",
      "        7.99954250e-04],\n",
      "       ...,\n",
      "       [1.14191162e-03, 1.03176459e-03, 4.33461703e-04, 2.83262170e-04,\n",
      "        3.04485912e-04],\n",
      "       [9.53813890e-04, 8.58497212e-04, 9.14528343e-04, 6.86652804e-04,\n",
      "        1.05175926e-03],\n",
      "       [8.35314174e-04, 2.39860872e-04, 2.09181507e-04, 1.91365792e-04,\n",
      "        5.37394701e-04]]), array([[4.72962477e-04, 3.62062127e-04, 4.77349895e-04, 6.82735154e-04,\n",
      "        3.79659738e-04],\n",
      "       [5.24745073e-05, 2.00939004e-04, 4.75926570e-04, 2.88202758e-04,\n",
      "        6.74201723e-04],\n",
      "       [1.17489047e-04, 1.82402609e-04, 4.21371333e-04, 5.41033939e-04,\n",
      "        7.08283859e-04],\n",
      "       ...,\n",
      "       [2.32574206e-04, 7.17213543e-05, 4.61197341e-04, 4.12266546e-04,\n",
      "        1.26761555e-04],\n",
      "       [5.14596577e-05, 1.36317808e-05, 4.01859367e-04, 3.64788327e-04,\n",
      "        1.46818371e-04],\n",
      "       [7.66448091e-04, 3.26851378e-04, 7.85697145e-05, 1.93930057e-04,\n",
      "        3.29050581e-04]]))\n",
      "-464937.351366529\n",
      "-422069.58596573304\n",
      "-421185.1700870958\n",
      "-420228.48827078054\n",
      "-419144.1580900826\n",
      "-418013.50773724954\n",
      "-416943.14586930035\n",
      "-415988.40010113985\n",
      "-415156.5758785611\n",
      "-414439.5991178481\n",
      "-413827.452039472\n",
      "-413304.68201583787\n",
      "-412857.4490086809\n",
      "-412470.9668629005\n",
      "-412132.92242715263\n",
      "-411832.3129830158\n",
      "-411558.7233421487\n",
      "-411303.3417576555\n",
      "-411061.13039068726\n",
      "-410837.4094196618\n",
      "-410633.125457377\n",
      "(array([0.19291421, 0.21856169, 0.22432333, 0.2338211 , 0.13037967]), array([[5.66870905e-06, 6.20325388e-07, 5.70165245e-10, 1.81536159e-03,\n",
      "        5.09837194e-12],\n",
      "       [2.93297096e-03, 4.74540361e-04, 5.67236618e-04, 2.04137271e-05,\n",
      "        3.89859714e-03],\n",
      "       [2.01273404e-04, 7.78584685e-10, 1.29851984e-03, 5.62330392e-04,\n",
      "        4.78109138e-04],\n",
      "       ...,\n",
      "       [1.63744019e-03, 9.80324460e-09, 1.58032278e-13, 3.44186182e-10,\n",
      "        8.42243520e-04],\n",
      "       [6.30555903e-14, 5.51320022e-04, 1.74447113e-04, 8.57822326e-04,\n",
      "        1.75332847e-13],\n",
      "       [4.08145323e-03, 2.10788038e-04, 7.15654861e-04, 8.66244386e-05,\n",
      "        6.86822998e-06]]), array([[2.10771793e-03, 1.37433360e-03, 6.01583557e-03, 1.10205890e-02,\n",
      "        5.65298500e-03],\n",
      "       [1.61679401e-11, 5.33558360e-29, 2.48650246e-09, 7.00235662e-04,\n",
      "        1.07757202e-16],\n",
      "       [1.35474414e-25, 2.27528300e-15, 9.35095031e-29, 9.80333285e-04,\n",
      "        7.09409206e-22],\n",
      "       ...,\n",
      "       [5.89762955e-36, 4.54353757e-27, 1.65996642e-33, 4.20142836e-04,\n",
      "        5.55722821e-30],\n",
      "       [1.18931908e-07, 4.73518606e-06, 2.40309371e-22, 1.24976574e-16,\n",
      "        4.94205131e-04],\n",
      "       [3.39488596e-04, 9.89487335e-26, 1.99718899e-22, 6.90472390e-23,\n",
      "        2.37895412e-33]]))\n"
     ]
    }
   ],
   "source": [
    "# EM-step several times\n",
    "pars = random_init_pars(K, n.shape)\n",
    "EMsteps(20, pars, docs2bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'human': 0,\n",
       " 'interface': 1,\n",
       " 'computer': 2,\n",
       " 'survey': 3,\n",
       " 'user': 4,\n",
       " 'system': 5,\n",
       " 'response': 6,\n",
       " 'time': 7,\n",
       " 'eps': 8,\n",
       " 'trees': 9,\n",
       " 'graph': 10,\n",
       " 'minors': 11}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now you can judge the result. We'll do it formally later\n",
    "token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Human machine interface for Lab ABC computer applications',\n",
       " 'A survey of user opinion of computer system response time',\n",
       " 'The EPS user interface management system',\n",
       " 'System and human system engineering testing of EPS',\n",
       " 'Relation of user-perceived response time to error measurement',\n",
       " 'The generation of random, binary, unordered trees',\n",
       " 'The intersection graph of paths in trees',\n",
       " 'Graph minors IV: Width of trees and well-quasi-ordering',\n",
       " 'Graph minors: A survey']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tempered Expectation step with control parameter beta\n",
    "def TEstep(beta, pars, docs2bow):\n",
    "    Pz, Pd_z, Pw_z = pars\n",
    "    posters = np.zeros((len(Pz), len(Pd_z), len(Pw_z)))\n",
    "    for z in range(len(Pz)):\n",
    "        for d, doc2bow in enumerate(docs2bow):\n",
    "            for w, ndw in doc2bow:\n",
    "                posters[z, d, w] = (Pz[z] * Pd_z[d, z] * Pw_z[w, z])**beta  # beta\n",
    "    posters /= posters.sum(axis=0) + 1e-16\n",
    "    return posters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tempered Maximization step\n",
    "def TMstep(posters, docs2bow):  # note beta has no role in TM-step. it played its role in TE-step\n",
    "    K, N, M = posters.shape\n",
    "    rePz, rePd_z, rePw_z = np.zeros(K), np.zeros((N, K)), np.zeros((M, K))\n",
    "    for z in range(K):\n",
    "        for d, doc2bow in enumerate(docs2bow):\n",
    "            for w, ndw in doc2bow:\n",
    "                rePz[z] += ndw * posters[z, d, w]\n",
    "                rePd_z[d, z] += ndw * posters[z, d, w]\n",
    "                rePw_z[w, z] += ndw * posters[z, d, w]\n",
    "    rePz /= sum(rePz)\n",
    "    rePd_z /= rePd_z.sum(axis=0)\n",
    "    rePw_z /= rePw_z.sum(axis=0)\n",
    "    repars = rePz, rePd_z, rePw_z\n",
    "    return repars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data to train and held-out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1440"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs2bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1)],\n",
       " [(3, 1), (4, 1), (2, 1), (5, 1), (6, 1), (7, 1)],\n",
       " [(8, 1), (4, 1), (1, 1), (5, 1)],\n",
       " [(5, 2), (0, 1), (8, 1)],\n",
       " [(4, 1), (6, 1), (7, 1)],\n",
       " [(9, 1)],\n",
       " [(10, 1), (9, 1)],\n",
       " [(10, 1), (11, 1), (9, 1)],\n",
       " [(10, 1), (11, 1), (3, 1)]]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs2bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over the corpus and randomly erase words\n",
    "# erased words will be writed in held-out corpus\n",
    "# unerased words remain in corpus as training corpus\n",
    "from numpy.random import randint\n",
    "\n",
    "docs2bow_train, docs2bow_heldout = list(), list()\n",
    "for doc2bow in docs2bow:\n",
    "    doc2bow_train, doc2bow_heldout = list(), list()\n",
    "    for w, ndw in doc2bow:\n",
    "        ndw_train = randint(ndw+1)\n",
    "        if ndw_train > 0:\n",
    "            doc2bow_train += [(w, ndw_train)]\n",
    "        if ndw - ndw_train > 0:\n",
    "            doc2bow_heldout += [(w, ndw - ndw_train)]\n",
    "    docs2bow_train += [(doc2bow_train)]\n",
    "    docs2bow_heldout += [(doc2bow_heldout)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1440"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs2bow_heldout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1)],\n",
       " [(4, 1), (5, 1), (6, 1)],\n",
       " [(8, 1), (4, 1), (5, 1)],\n",
       " [(5, 1), (0, 1), (8, 1)],\n",
       " [(4, 1), (6, 1), (7, 1)],\n",
       " [],\n",
       " [],\n",
       " [(10, 1), (11, 1)],\n",
       " [(11, 1)]]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs2bow_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1440"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs2bow_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(1, 1), (2, 1)],\n",
       " [(3, 1), (2, 1), (7, 1)],\n",
       " [(1, 1)],\n",
       " [(5, 1)],\n",
       " [],\n",
       " [(9, 1)],\n",
       " [(10, 1), (9, 1)],\n",
       " [(9, 1)],\n",
       " [(10, 1), (3, 1)]]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs2bow_heldout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEM for train and heldout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify likelihood to solve the problem of omitted words or docs\n",
    "# in splitting, may some words be omitted entirely from training corpus\n",
    "# therefore their condit probs P(w|z) remain zero in training procedure\n",
    "# why? note that EM-steps works only with training corpus and obviously omitted words left unseen in training\n",
    "# and also note that repars accumulate from zero for \"seen\" data, so repars for unseen data remain zero through M-step\n",
    "# so in evaluating performance on held-out data by likelihood(pars, heldout), Pcocur(omitted) would be 0 (P(w|z) = 0)\n",
    "# and it diverges log-likelihood!\n",
    "# so for avoiding this problem, we ignore omitted words in log-likelihood calculations\n",
    "# similar problem could happen for omitted docs\n",
    "def likelihood(pars, docs2bow):\n",
    "    Pz, Pd_z, Pw_z = pars\n",
    "    L = 0\n",
    "    for d, doc2bow in enumerate(docs2bow):\n",
    "        for w, ndw in doc2bow:\n",
    "            Pcocur = sum(Pz[:] * Pd_z[d,:] * Pw_z[w, :])\n",
    "            # modification\n",
    "            if Pcocur == 0: continue\n",
    "            L += ndw * np.log(Pcocur)\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEM-steps on training corpus and evaluating performance by likelihood on held-out corpus\n",
    "def TEMsteps(beta_runtimes, first_beta, eta, docs2bow_train, docs2bow_heldout):\n",
    "    beta = first_beta\n",
    "    # pars = random_init_pars(K, n.shape)\n",
    "    pars = random_init_pars(K, (len(docs2bow), len(token2id)))\n",
    "    for i in range(beta_runtimes):\n",
    "        print(beta)\n",
    "        # pars = random_init_pars(K, n.shape)\n",
    "        # print(pars)\n",
    "        new_likeli = likelihood(pars, docs2bow_heldout) # first likelihood in new beta\n",
    "        likeli = 2 * new_likeli    # for assuring the entrance to while loop\n",
    "        # while round(new_likeli, 0) > round(likeli, 0):  # check if likelihood increased?\n",
    "        while (new_likeli - likeli) / likeli < -0.0001:  # define a relative condition\n",
    "            likeli = new_likeli\n",
    "            print(likeli)   # print increased likelihood (or first likelihood)\n",
    "            # one TEM step\n",
    "            prepars = pars  # save pars before executing TEM, for undoing pars if khodayi nakarde likelihood decreased\n",
    "            posters = TEstep(beta, pars, docs2bow_train)    # TE-step\n",
    "            pars = TMstep(posters, docs2bow_train)  # TM-step\n",
    "            # print(pars)\n",
    "            new_likeli = likelihood(pars, docs2bow_heldout) # calculating likelihood for re-estimated pars\n",
    "        print(new_likeli)   # print decreased (inappropriate) likelihood\n",
    "        pars = prepars  # undo pars\n",
    "        beta *= eta # new beta\n",
    "    return pars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "-142034.84801177596\n",
      "-114597.9118182885\n",
      "-114744.60556501515\n",
      "0.9\n",
      "-114597.9118182885\n",
      "-114681.07421818357\n",
      "0.81\n",
      "-114597.9118182885\n",
      "-114635.51834282668\n",
      "0.7290000000000001\n",
      "-114597.9118182885\n",
      "-114603.31951495273\n",
      "0.6561000000000001\n",
      "-114597.9118182885\n",
      "-114580.85690436112\n",
      "-114571.01725802285\n",
      "0.5904900000000002\n",
      "-114580.85690436112\n",
      "-114554.34638887564\n",
      "-114537.22356496235\n",
      "-114526.09642314336\n",
      "0.5314410000000002\n",
      "-114537.22356496235\n",
      "-114517.25843671068\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-89-db6779cdded5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# to find the approp beta, try TEM for various betas, on training and held-out corpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mTEMsteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.00\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.90\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocs2bow_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocs2bow_heldout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-88-4cb130086600>\u001b[0m in \u001b[0;36mTEMsteps\u001b[0;34m(beta_runtimes, first_beta, eta, docs2bow_train, docs2bow_heldout)\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mprepars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpars\u001b[0m  \u001b[0;31m# save pars before executing TEM, for undoing pars if khodayi nakarde likelihood decreased\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mposters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTEstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocs2bow_train\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# TE-step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0mpars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTMstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocs2bow_train\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# TM-step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m             \u001b[0;31m# print(pars)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mnew_likeli\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlikelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocs2bow_heldout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# calculating likelihood for re-estimated pars\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-85-12b811d7d2ac>\u001b[0m in \u001b[0;36mTMstep\u001b[0;34m(posters, docs2bow)\u001b[0m\n\u001b[1;32m      8\u001b[0m                 \u001b[0mrePz\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mndw\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mposters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                 \u001b[0mrePd_z\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mndw\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mposters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                 \u001b[0mrePw_z\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mndw\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mposters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mrePz\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrePz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mrePd_z\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mrePd_z\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# to find the approp beta, try TEM for various betas, on training and held-out corpus\n",
    "TEMsteps(20, 1.00, 0.90, docs2bow_train, docs2bow_heldout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final TEM by appropriate beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6\n",
      "-284971.88829544524\n",
      "-268684.7852031237\n",
      "-268614.0707105244\n",
      "-268652.8475304686\n"
     ]
    }
   ],
   "source": [
    "# to obtain the learned pars, one TEM by approp beta, on the whole corpus\n",
    "tem_learned_pars = TEMsteps(1, 0.6, 0.00, docs2bow, docs2bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "-285288.4394184393\n",
      "-267813.03912732063\n",
      "-265702.0525638421\n",
      "-262431.01265061455\n",
      "-258045.55771821545\n",
      "-253556.00927200477\n",
      "-249984.59537374915\n",
      "-247477.68256215408\n",
      "-245704.39277598343\n",
      "-244398.41420856086\n",
      "-243414.15568376673\n",
      "-242662.37876061452\n",
      "-242071.4543976675\n",
      "-241592.91907326685\n",
      "-241206.6328143457\n",
      "-240907.64427584526\n",
      "-240676.65556688447\n",
      "-240486.09137127933\n",
      "-240319.65002794177\n",
      "-240172.4842573757\n",
      "-240040.83141388625\n",
      "-239924.7322200468\n",
      "-239818.5380009141\n",
      "-239720.330684687\n",
      "-239628.9580664988\n",
      "-239542.05817770082\n",
      "-239469.77303903268\n",
      "-239408.1277675109\n",
      "-239351.10913901663\n",
      "-239296.27930801772\n",
      "-239241.8001750799\n",
      "-239193.54131252322\n",
      "-239150.59123493123\n",
      "-239108.61459593775\n",
      "-239069.07533886583\n",
      "-239033.01205629032\n",
      "-238997.55811592986\n",
      "-238959.68848476443\n",
      "-238921.90771125632\n",
      "-238886.6331557354\n",
      "-238855.2269356264\n",
      "-238826.43392605553\n",
      "-238797.813693915\n",
      "-238770.08775136797\n",
      "-238742.50516290125\n",
      "-238716.34175785256\n",
      "-238693.9971708892\n"
     ]
    }
   ],
   "source": [
    "# one EM (TEM by beta = 1), on the whole corpus\n",
    "em_learned_pars = TEMsteps(1, 1.00, 0.00, docs2bow, docs2bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2token = {tokenid: token for token, tokenid in token2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'human',\n",
       " 1: 'interface',\n",
       " 2: 'computer',\n",
       " 3: 'survey',\n",
       " 4: 'user',\n",
       " 5: 'system',\n",
       " 6: 'response',\n",
       " 7: 'time',\n",
       " 8: 'eps',\n",
       " 9: 'trees',\n",
       " 10: 'graph',\n",
       " 11: 'minors'}"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P(w|z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pprint\n",
    "\n",
    "def token2Pw_z(pars, id2token, first_probs=None):\n",
    "    Pz, Pd_z, Pw_z = pars\n",
    "    for z in range(len(Pz)):\n",
    "        token2Pw_z = {id2token[tokenid]: prob for tokenid, prob in enumerate(Pw_z[:, z])}\n",
    "        sorted_token2Pw_z = dict(sorted(token2Pw_z.items(), key=lambda item: -item[1])[:first_probs])\n",
    "        print(f'P(w|z={z}) = {sorted_token2Pw_z}\\n')\n",
    "        # pprint.pprint(sorted_token2Pw_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(w|z=0) = {'ebola': 0.027648390464278125, 'nhs': 0.022792457862969993, 'care': 0.012495959350300212, 'audio': 0.010957717665466441, 'health': 0.010206425415155884, 'hospital': 0.009480355076725224, 's': 0.008716434315693094, 'over': 0.00861416786580953, 'patients': 0.007894588007376904, 'amp': 0.006880905874243768, 'e': 0.006530517360499943, 'be': 0.0064264357580104845, 'new': 0.006406982748547574, 'risk': 0.00633922889990938, 'uk': 0.006096585924608676, 'call': 0.005702676330427416, 'child': 0.005591937530943784, 'mental': 0.0050070047400813675, 'help': 0.0049528568887827785, 'may': 0.004927010574704252}\n",
      "\n",
      "P(w|z=1) = {'ebola': 0.022687556086690056, 'nhs': 0.022553664685959523, 'cancer': 0.01875419828543511, 'health': 0.012882196768281118, 'new': 0.011185796077352825, 'over': 0.008421344890084071, 'audio': 0.008076736108589467, 's': 0.00806834783743448, 'e': 0.00707670620214765, 'patients': 0.006962451551150965, 'amp': 0.006941114003582712, 'mental': 0.006234011338789937, 'be': 0.005554525091145982, 'with': 0.0055121113103260575, 'can': 0.005493302928821259, 'drug': 0.005161132678490014, 'more': 0.004575891735192736, 'hospital': 0.0044226211910251, 'death': 0.004398457265477021, 'obesity': 0.00436206656128292}\n",
      "\n",
      "P(w|z=2) = {'nhs': 0.022945001060946497, 'cancer': 0.01590655792597658, 'ebola': 0.014400495482400523, 'care': 0.013401910716616587, 'audio': 0.011985940774363725, 'over': 0.011313265223293529, 's': 0.010170359353648243, 'new': 0.008155692984027602, 'health': 0.007969842907556675, 'hospital': 0.0073914421460605385, 'call': 0.007067964633238166, 'e': 0.005672247486332438, 'risk': 0.005534056689338353, 'at': 0.005115969906477322, 'heart': 0.0049754638711401655, 'mental': 0.004924762644009412, 'not': 0.00478397241739596, 'how': 0.004709197461286261, 'gp': 0.004663815228332682, 'children': 0.004632684527863134}\n",
      "\n",
      "P(w|z=3) = {'s': 0.01565193186316427, 'cancer': 0.015272641861915135, 'health': 0.013562055799390082, 'nhs': 0.013232186349550201, 'audio': 0.011584634331439886, 'over': 0.011077003776224531, 'ebola': 0.010022508736216962, 'care': 0.008119770969941377, 'e': 0.007826289624242005, 'hospital': 0.0074115048252265435, 'call': 0.006074616163282853, 'at': 0.005492284150720356, 'death': 0.0054769524053370315, 'may': 0.005435608760457556, 'children': 0.005377400205767324, 'risk': 0.005105153575068606, 'can': 0.00503894344645552, 'not': 0.0049695036467146715, 'life': 0.0046266366327019975, 'obesity': 0.004202285105693234}\n",
      "\n",
      "P(w|z=4) = {'nhs': 0.02357031742677984, 'cancer': 0.016681374799831884, 's': 0.014911457620218489, 'ebola': 0.011811199234111757, 'health': 0.010766010164811018, 'audio': 0.01009826776439401, 'over': 0.010034563630953484, 'care': 0.009795403829368223, 'e': 0.009076591430645892, 'hospital': 0.008205644810490192, 'uk': 0.007557459561699187, 'mental': 0.006821983236340215, 'new': 0.006385280622592624, 'amp': 0.006345283609948193, 'call': 0.005558565978556526, 'more': 0.004951322069050672, 'be': 0.004567492106832118, 'not': 0.00455170968713684, 'may': 0.004531805741670555, 'brain': 0.004232924205092739}\n",
      "\n",
      "P(w|z=5) = {'ebola': 0.01989501531409561, 's': 0.013297831242855941, 'health': 0.0120012125627, 'hospital': 0.010394065576500237, 'care': 0.010056985008893921, 'new': 0.009293545179153238, 'audio': 0.00898895946336197, 'cancer': 0.00856554515978911, 'amp': 0.007651425972372296, 'nhs': 0.00739885909365664, 'be': 0.0071890066548242855, 'over': 0.007051838565439642, 'death': 0.006491736026391703, 'can': 0.005936804001978586, 'help': 0.005674842235163315, 'mental': 0.005653855916400722, 'drug': 0.0054648160204141165, 'children': 0.005315315241904989, 'e': 0.0046672530335452435, 'life': 0.004469702653456612}\n",
      "\n",
      "P(w|z=6) = {'nhs': 0.027644175138730912, 'ebola': 0.017766961471137207, 'cancer': 0.01751786665867504, 's': 0.014724290559395043, 'hospital': 0.011572825938863938, 'audio': 0.011341553322223131, 'health': 0.010286581606236198, 'care': 0.00994004812817077, 'over': 0.009206666874108269, 'e': 0.008095847600858743, 'new': 0.007955474153563798, 'patients': 0.007379280613608887, 'be': 0.0069573075663018616, 'more': 0.005487412970040478, 'uk': 0.005269189466477634, 'can': 0.0048635777722594005, 'life': 0.004839861823939252, 'baby': 0.004421174348009108, 'risk': 0.004416541707274897, 'not': 0.004295532501139015}\n",
      "\n",
      "P(w|z=7) = {'nhs': 0.019699374137576373, 'ebola': 0.015155344041943911, 's': 0.01353341766767993, 'cancer': 0.012686317317845561, 'care': 0.012416283435767523, 'hospital': 0.01050909375505378, 'over': 0.008060314549605832, 'health': 0.006604035552762932, 'children': 0.006491613082226373, 'risk': 0.006217931210961947, 'e': 0.005644167253723483, 'not': 0.005584993635003382, 'audio': 0.005390906259397976, 'help': 0.005282589926753691, 'death': 0.00515992687837234, 'uk': 0.005090699188933882, 'at': 0.005079409274265122, 'can': 0.004712878777371847, 'call': 0.004525125397601215, 'mental': 0.004409500047524437}\n",
      "\n",
      "P(w|z=8) = {'ebola': 0.01629894594791113, 'cancer': 0.015585258808661689, 'care': 0.014599300739120585, 'nhs': 0.014393090261587065, 's': 0.013744868776681731, 'health': 0.013656105136180109, 'over': 0.009694602911098417, 'new': 0.00964124471913695, 'audio': 0.009171081134863427, 'e': 0.00912619352158172, 'mental': 0.007002649561679359, 'hospital': 0.006968463703773255, 'patients': 0.006717785413567098, 'risk': 0.0062257119663667836, 'children': 0.006115855082165671, 'call': 0.005463334524909876, 'amp': 0.005299108291853581, 'not': 0.005042853680490731, 'more': 0.0050194046937989244, 'can': 0.004885896738718037}\n",
      "\n",
      "P(w|z=9) = {'ebola': 0.02151673470141287, 'nhs': 0.021135525551286047, 'cancer': 0.016587729491653627, 'health': 0.012236112416064298, 'care': 0.011194876737587721, 's': 0.010796553934472439, 'audio': 0.009311253092928375, 'new': 0.008469361684606524, 'hospital': 0.0075323864309418224, 'uk': 0.007249024914494342, 'over': 0.006858409154853636, 'amp': 0.006660828591895021, 'call': 0.006193190218705824, 'e': 0.006024910227077675, 'mental': 0.005729605657912175, 'more': 0.005590589011210636, 'with': 0.005127933970521478, 'patients': 0.0050205700421135716, 'can': 0.004940231188390088, 'at': 0.00447984385954595}\n",
      "\n",
      "P(w|z=10) = {'nhs': 0.02735467098618916, 'ebola': 0.024476395559212653, 's': 0.012827394705931803, 'care': 0.012569273821302284, 'cancer': 0.011767480685714196, 'health': 0.010099349192634737, 'audio': 0.00878592236281432, 'e': 0.007959432504897859, 'risk': 0.006305309729355326, 'patients': 0.005928812789929957, 'hospital': 0.005564927655629382, 'how': 0.00554565594937112, 'uk': 0.0054421533107866255, 'child': 0.005382961262475591, 'more': 0.005309718520704204, 'be': 0.005135005056562685, 'at': 0.004973449893246494, 'not': 0.004805909617091742, 'over': 0.004786742179068424, 'children': 0.004580810439148304}\n",
      "\n",
      "P(w|z=11) = {'nhs': 0.021276712189629773, 's': 0.013910264984738845, 'new': 0.01139248959545815, 'cancer': 0.010721227723290153, 'e': 0.010127200605156308, 'over': 0.0075023827166730344, 'uk': 0.007303540274335566, 'care': 0.006865819229063273, 'health': 0.006860433648038742, 'amp': 0.00680750852851895, 'mental': 0.006760372019925298, 'children': 0.00669739037360972, 'audio': 0.0066259909915927255, 'ebola': 0.006371733175724661, 'hospital': 0.006154315304109012, 'how': 0.0058234682912393886, 'be': 0.005418420138634523, 'call': 0.0053163981173841085, 'death': 0.005146340119161458, 'by': 0.004509656996740718}\n",
      "\n",
      "P(w|z=12) = {'ebola': 0.026845131840225245, 'nhs': 0.0187246171026336, 'new': 0.012077620445302051, 'care': 0.011959564211928784, 'audio': 0.011542479247407453, 'health': 0.010774604495909702, 'cancer': 0.009585708491345948, 'over': 0.009044350711045552, 'e': 0.007957893678645208, 'amp': 0.0071293454585149635, 'be': 0.00699879349729051, 's': 0.006844054342757869, 'hospital': 0.0065269106477234525, 'drug': 0.005754928629275873, 'may': 0.005165556245500935, 'life': 0.004690215289507878, 'mental': 0.004610783449498542, 'risk': 0.004219474470890221, 'could': 0.004205439059391529, 'heart': 0.004165903682951214}\n",
      "\n",
      "P(w|z=13) = {'ebola': 0.026929344875570894, 'nhs': 0.01729929187838062, 'health': 0.01333086627674532, 'care': 0.010668208282092157, 'over': 0.010133680891944125, 's': 0.009198765568539971, 'e': 0.0089966968546074, 'new': 0.00845412917022206, 'uk': 0.007377799119769311, 'cancer': 0.006841083344664723, 'amp': 0.006595613439843229, 'be': 0.0061912844238200815, 'call': 0.006025538666937051, 'help': 0.005861724132492955, 'death': 0.005199552550722139, 'at': 0.0050674682154335845, 'more': 0.004845436627591495, 'can': 0.004771823571027065, 'not': 0.004699538584052786, 'could': 0.00464914234875452}\n",
      "\n",
      "P(w|z=14) = {'ebola': 0.0267467595481241, 'health': 0.013730415447345684, 's': 0.012415053691894365, 'audio': 0.010577904294847002, 'risk': 0.00810764870771371, 'hospital': 0.00763663919076765, 'cancer': 0.0075176678425937, 'e': 0.007162110615172099, 'over': 0.007050477264028696, 'care': 0.006743967995389958, 'mental': 0.0058711238302065726, 'uk': 0.005783546181185546, 'with': 0.005761715839964709, 'new': 0.00556482494749998, 'heart': 0.005513327906210251, 'be': 0.0052989211849932055, 'at': 0.00523799509130183, 'child': 0.005209975789445723, 'patients': 0.004955735543527183, 'baby': 0.00484251369983601}\n",
      "\n",
      "P(w|z=15) = {'nhs': 0.0269913619491005, 'ebola': 0.014694405263813118, 'cancer': 0.012634155328604683, 's': 0.011525916466999965, 'over': 0.011420671437916901, 'health': 0.011176247213538378, 'care': 0.011152296424878645, 'hospital': 0.009647518371795628, 'audio': 0.008160823815153137, 'uk': 0.0075793499543734795, 'patients': 0.007037812353307328, 'be': 0.00602173371921706, 'amp': 0.005888542787937058, 'mental': 0.0058771835407943835, 'help': 0.00558821546265094, 'drug': 0.005350556621075356, 'risk': 0.0050457569356910335, 'more': 0.0050141604751857615, 'at': 0.0048724163206060956, 'with': 0.004867766221250807}\n",
      "\n",
      "P(w|z=16) = {'ebola': 0.01828417216351042, 's': 0.014694142945648898, 'nhs': 0.013906655520906681, 'cancer': 0.012911826775803182, 'audio': 0.011597488061327148, 'care': 0.00888219978289285, 'patients': 0.008323672842623024, 'e': 0.008318013885641547, 'hospital': 0.006626440463816327, 'amp': 0.006355217033386342, 'over': 0.006299734090739325, 'drug': 0.006197493905029222, 'mental': 0.005550327117518239, 'health': 0.005120561522736044, 'more': 0.005113452005822164, 'not': 0.005092679294548113, 'death': 0.004988621228270093, 'may': 0.004729667990076754, 'uk': 0.0044791605653857255, 'children': 0.004379496646657075}\n",
      "\n",
      "P(w|z=17) = {'ebola': 0.025800677595317203, 'nhs': 0.023863888711438946, 'health': 0.015444036029846587, 's': 0.01261074565485294, 'care': 0.011554496480254615, 'cancer': 0.011344395940969738, 'audio': 0.00934128729768585, 'new': 0.00883703662064284, 'amp': 0.0068424428474801536, 'mental': 0.006496399217209667, 'uk': 0.0061519831983264, 'not': 0.005813705193788416, 'help': 0.005556052796976043, 'how': 0.005224704439200254, 'e': 0.005145312474254988, 'drug': 0.005082776639953773, 'children': 0.004998000809606642, 'can': 0.0049879266038141525, 'at': 0.0049852366825156305, 'child': 0.00496756144100006}\n",
      "\n",
      "P(w|z=18) = {'nhs': 0.025208355364704865, 'ebola': 0.020725469837828556, 'cancer': 0.016491073658906553, 's': 0.014584076647502296, 'health': 0.01359730637320934, 'hospital': 0.011740864215558948, 'care': 0.010541217929516795, 'e': 0.007321354769167259, 'audio': 0.007208572854790412, 'uk': 0.006959809546525924, 'patients': 0.006836150094434481, 'new': 0.006613664487427222, 'risk': 0.006318545466058303, 'amp': 0.00587737076078957, 'be': 0.005391611127079543, 'drug': 0.005241084398401497, 'warning': 0.004881565018870985, 'help': 0.0047551961199736575, 'how': 0.004701839792640412, 'not': 0.0046168826376795195}\n",
      "\n",
      "P(w|z=19) = {'ebola': 0.025731122256721445, 's': 0.014730167147247956, 'nhs': 0.010977161382317543, 'cancer': 0.009753421382945703, 'hospital': 0.009123013931190966, 'uk': 0.007897394689926734, 'risk': 0.007025845958503684, 'audio': 0.006713410934800543, 'drug': 0.006549064173514257, 'over': 0.0064642861263678785, 'death': 0.005684596193682326, 'help': 0.00535999331188329, 'more': 0.005111344393136965, 'health': 0.004990697693163165, 'with': 0.004984334179542611, 'children': 0.004749548554071018, 'call': 0.004727523448427909, 'can': 0.004725149735087031, 'be': 0.0046856482879288255, 'care': 0.004615276312437075}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "token2Pw_z(tem_learned_pars, id2token, first_probs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(w|z=0) = {'vaccines': 0.03247460585596786, 'in': 0.03170834945392084, 'covid': 0.02855633930916494, '19': 0.027949705922424323, 'is': 0.015350104962010456, 'doses': 0.014929734914714329, 'first': 0.013026888964876386, 'have': 0.011130983926283833, 'vaccine': 0.010408665479738401, '#covax': 0.010092831511789885, 'unicef': 0.009943940401509244, 'with': 0.009400984150527168, 'through': 0.009339046359508217, 'covax': 0.009133484885749813, 'countries': 0.008779356187299407, 'this': 0.00875529302862728, 'are': 0.008734285537327227, '#covid19': 0.008360411248748234, 'today': 0.008251810107146491, 'has': 0.007948913906560807}\n",
      "\n",
      "P(w|z=1) = {'in': 0.03137692864558328, 'year': 0.021847861682374643, 'her': 0.020524772888915393, 'is': 0.016656840376821865, 'old': 0.016008465075770438, 'with': 0.014949047723987629, 'water': 0.013281882248494383, 'must': 0.01293388765432907, 'from': 0.012294802532501177, 'are': 0.012027752831292522, 'we': 0.011575922857492607, 'that': 0.011538450841227366, 'schools': 0.009281152586724064, 'at': 0.009000666525662816, 'million': 0.008547110927644768, 'was': 0.008246148094551898, 'have': 0.007946176086723211, 'world': 0.007908770425965893, '”': 0.0078078772667708645, 'now': 0.007748819663243677}\n",
      "\n",
      "P(w|z=2) = {'we': 0.03326425621666424, 'is': 0.023640106495754553, 'in': 0.01901532940144907, 'not': 0.014494384001349102, 'water': 0.01334222686464166, 'are': 0.013187841826180723, 'can': 0.012809202639616145, 'world': 0.012277068961629362, 'children': 0.012249694000188409, 'child': 0.011813525811069888, 'all': 0.011248189789583833, 'every': 0.011113507683130353, 'must': 0.011088222248344331, 'on': 0.010097823010774342, '”': 0.009990146881582852, 'education': 0.0099479546744648, 'from': 0.00958010701960586, 'an': 0.009276733003682269, '@unicefchief': 0.008811545213051166, 'future': 0.008732410060330572}\n",
      "\n",
      "P(w|z=3) = {'covid': 0.03129165440179216, '19': 0.03095847429466447, 'is': 0.01937303513148107, 'how': 0.015033539096705793, 'health': 0.014236005136772246, 'are': 0.01272921943368296, 'you': 0.01242221537767656, 'our': 0.012198627399814432, 'on': 0.012145201053271147, 'pandemic': 0.011498292928195025, 'as': 0.01098412124835098, 'in': 0.010853346526410228, 'can': 0.010487867674359387, 'from': 0.010322226868153376, 'school': 0.010245721571796977, 'your': 0.010138287644400865, 'out': 0.010084430716204759, 'during': 0.009696918515854575, 'children': 0.00901992857849116, 'their': 0.008876241797529506}\n",
      "\n",
      "P(w|z=4) = {'in': 0.0565365836261251, 'children': 0.038846976875837644, 'is': 0.023045517631768195, 'on': 0.014217203262599211, 'this': 0.013130915923270016, '@unicefchief': 0.013003569755903471, 'are': 0.01249303950145908, 'unicef': 0.01011691335038014, 'families': 0.010094207831146342, 'violence': 0.009177835513396355, 'all': 0.009037580548575239, 'by': 0.008361361865842505, 'at': 0.008225403151472277, 'their': 0.007994921121169831, 'have': 0.007231215087477448, 'conflict': 0.006477661314929099, 'has': 0.006407371511253075, 'need': 0.006210192291102683, 'as': 0.006207437666486924, 'people': 0.00592782895579005}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "token2Pw_z(em_learned_pars, id2token, first_probs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P(w|d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc2token(docno, id2token, docs2bow):\n",
    "    converted_doc = {id2token[tokenid]: docfreq for tokenid, docfreq in docs2bow[docno]}\n",
    "    return converted_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'covid': 1,\n",
       " '19': 1,\n",
       " 'disrupted': 1,\n",
       " 'education': 2,\n",
       " 'around': 1,\n",
       " 'world': 1,\n",
       " 'but': 1,\n",
       " 'there’s': 1,\n",
       " 'always': 1,\n",
       " 'way': 1,\n",
       " 'keep': 1,\n",
       " 'children': 3,\n",
       " 'learning': 2,\n",
       " 'in': 3,\n",
       " '2020': 1,\n",
       " 'unicef': 1,\n",
       " 'reached': 1,\n",
       " '48': 1,\n",
       " 'million': 1,\n",
       " 'out': 1,\n",
       " 'school': 2,\n",
       " 'with': 1,\n",
       " 'early': 1,\n",
       " 'primary': 1,\n",
       " 'amp': 1,\n",
       " 'secondary': 1,\n",
       " 'now': 1,\n",
       " '2021': 1,\n",
       " 'we’re': 1,\n",
       " 'working': 1,\n",
       " 'get': 1,\n",
       " 'safely': 1,\n",
       " 'back': 1}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2token(1, id2token, docs2bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc2Pw_d(docno, pars, id2token, docs2bow):\n",
    "    Pz, Pd_z, Pw_z = pars\n",
    "    Pdz = Pz * Pd_z\n",
    "    Pz_d = (Pdz.T / Pdz.sum(axis=1)).T  # P(z|d) = Pz_d[d, z]\n",
    "    Pw_docno = (Pz_d[docno] * Pw_z).sum(axis=1)\n",
    "    # translate and sort Pw_docno\n",
    "    token2Pw_docno = {id2token[tokenid]: prob for tokenid, prob in enumerate(Pw_docno)}\n",
    "    sorted_token2Pw_docno = dict(sorted(token2Pw_docno.items(), key=lambda item: -item[1])[:2*len(docs2bow[docno])]) # length\n",
    "    print(doc2token(docno, id2token, docs2bow))\n",
    "    print(sorted_token2Pw_docno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'@unicefindia': 1, '#unicef': 1, 'supports': 1, 'india’s': 1, '#covid19': 1, 'emergency': 1, 'response': 1, 'critical': 1, 'lifesaving': 1, 'supplies': 1, 'will': 1, 'help': 1, 'fight': 1, 'against': 1}\n",
      "{'vaccines': 0.03247460584737785, 'in': 0.031708349461713574, 'covid': 0.02855633930141877, '19': 0.027949705915046728, 'is': 0.01535010496442578, 'doses': 0.014929734910028398, 'first': 0.013026888960787693, 'have': 0.01113098392505983, 'vaccine': 0.010408665476693859, '#covax': 0.010092831508622092, 'unicef': 0.009943940401563534, 'with': 0.009400984148924412, 'through': 0.009339046357002237, 'covax': 0.009133484882883127, 'countries': 0.00877935618563042, 'this': 0.008755293030000636, 'are': 0.00873428553850697, '#covid19': 0.008360411246128989, 'today': 0.008251810105222262, 'has': 0.007948913906076969, 'now': 0.007413839305961486, 'health': 0.0072169582039948625, '000': 0.007136728428247235, 'work': 0.0068405383705121454, 'will': 0.006581083396570647, 'more': 0.006300083062627015, 'arrived': 0.006293660760995741, 'workers': 0.006164119047105086}\n"
     ]
    }
   ],
   "source": [
    "doc2Pw_d(10, em_learned_pars, id2token, docs2bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter out more tokens from corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_tokens = set('is are in on from an'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'an', 'are', 'from', 'in', 'is', 'on'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_ids = set(token2id[bad_token] for bad_token in bad_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{24, 45, 68, 75, 150, 295}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "874.0"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n[:, 24].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter token2id\n",
    "token2id = {token: tokenid for token, tokenid in token2id.items() if tokenid not in bad_ids}\n",
    "# creating idmap\n",
    "idmap = dict(zip(sorted(token2id.values()), range(len(token2id))))\n",
    "# compactify token2id\n",
    "token2id = {token: idmap[tokenid] for token, tokenid in token2id.items()}\n",
    "# id2token\n",
    "id2token = {tokenid: token for token, tokenid in token2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rebuild filtered docs2bow\n",
    "docs2bow = [\n",
    "    [(idmap[tokenid], docfreq) for tokenid, docfreq in doc2bow if tokenid not in bad_ids]\n",
    "    for doc2bow in docs2bow\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train again by new filtered corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75\n",
      "-425431.66268389224\n",
      "-391439.11257212405\n",
      "-391138.267289122\n",
      "-390969.0717964464\n",
      "-390818.6538914301\n",
      "-390650.4862082611\n",
      "-390448.5779116147\n",
      "-390204.582284762\n",
      "-389913.7764248532\n",
      "-389573.5008412661\n",
      "-389182.69413731724\n",
      "-388742.18318297184\n",
      "-388255.2727892071\n",
      "-387728.0546009698\n",
      "-387169.10366643424\n",
      "-386588.63671553467\n",
      "-385997.4402678283\n",
      "-385405.90921703994\n",
      "-384823.41192367853\n",
      "-384258.0258708974\n",
      "-383716.63024695887\n",
      "-383205.12450906227\n",
      "-382728.21987988666\n",
      "-382288.62353162246\n",
      "-381886.39245234616\n",
      "-381519.243740481\n",
      "-381183.6785522993\n",
      "-380876.30958475795\n",
      "-380594.8119565753\n",
      "-380337.8152701607\n",
      "-380103.8778292938\n",
      "-379890.8424754466\n",
      "-379696.19332616864\n",
      "-379517.6557088851\n",
      "-379353.25884685153\n",
      "-379201.12307417206\n",
      "-379059.4064489057\n",
      "-378926.4659374031\n",
      "-378801.1888155188\n",
      "-378683.1974101788\n",
      "-378572.5376312022\n",
      "-378469.1387275922\n",
      "-378372.6534928521\n",
      "-378282.6427066944\n",
      "-378198.6882268026\n",
      "-378120.3101748087\n",
      "-378046.91194053355\n",
      "-377977.83030696196\n",
      "-377912.3938625884\n",
      "-377849.9373187074\n",
      "-377789.78956861724\n",
      "-377731.2984493846\n",
      "-377673.9572257804\n",
      "-377617.6036727075\n",
      "-377562.55543920345\n",
      "-377509.5602769825\n",
      "-377459.4812616523\n",
      "-377412.8504948979\n",
      "-377369.6725031498\n",
      "-377329.59018056776\n",
      "-377292.13149586745\n"
     ]
    }
   ],
   "source": [
    "tem_learned_pars = TEMsteps(1, 0.75, 0.00, docs2bow, docs2bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(w|z=0) = {'covid': 0.02468356647913886, '19': 0.024253387009451437, 'health': 0.022261930699514716, 'children': 0.01969548855770016, 'as': 0.014908684959769932, 'their': 0.014148953752200037, 'we': 0.014108597642450538, 'need': 0.0134506872765165, 'with': 0.01271699538036316, 'support': 0.010442469174984214, 'our': 0.009198552431727314, 'unicef': 0.008986724080209932, 'protect': 0.008824943871514635, 'this': 0.008752493028241057, 'pandemic': 0.00806233140197669}\n",
      "\n",
      "P(w|z=1) = {'we': 0.02369161019191525, 'children': 0.018073847248061034, 'water': 0.016281902600475736, 'education': 0.015550203128473423, 'must': 0.013947298068170222, 'world': 0.0127433691651005, 'schools': 0.010981692987583705, 'year': 0.01096883433121957, 'learning': 0.01049214327022325, 'child': 0.0101754646051107, '@unicefchief': 0.009806186236152769, 'school': 0.009777940250866196, 'can': 0.009587456818836222, 'her': 0.009584724517253985, 'they': 0.009276802224399649}\n",
      "\n",
      "P(w|z=2) = {'our': 0.01532605410362877, 'you': 0.01465501970814741, 'how': 0.013559598596878712, 'about': 0.012648298541088598, '@voicesofyouth': 0.01107147072363525, 'can': 0.011003715769385986, 'young': 0.010608066285023606, 'i': 0.010383754157336514, 'out': 0.010234490459789657, 'we': 0.009936618780608631, 'your': 0.009306206222414456, '19': 0.009280379451584842, 'what': 0.009220773921787299, 'covid': 0.009062129261645947, 'people': 0.008724948690431559}\n",
      "\n",
      "P(w|z=3) = {'children': 0.0385715601010198, 'have': 0.016667232853596888, 'at': 0.012067087929872105, 'violence': 0.012031855378188596, '@unicefchief': 0.011636593076845439, '000': 0.011238918392005021, 'by': 0.01096346614726646, 'doses': 0.009855805605151117, 'unicef': 0.009273633340105475, 'with': 0.009226479086079459, 'today': 0.009090773119118704, 'killed': 0.008091318768063903, 'amp': 0.00806599966923439, 'this': 0.007872023663096087, 'million': 0.007848933099066185}\n",
      "\n",
      "P(w|z=4) = {'covid': 0.04027912856885494, '19': 0.04017549446980176, 'vaccines': 0.03918410498561141, 'vaccine': 0.01866075006689478, 'this': 0.01625056057817171, 'first': 0.013200593502250186, 'countries': 0.011996606962440886, '@unicefchief': 0.011803828704984051, 'will': 0.010422681764855746, 'unicef': 0.0096229466786534, '#covid19': 0.009353221158138084, 'covax': 0.008799044894682816, 'all': 0.008430087252184918, 'have': 0.008144102043558803, 'with': 0.008094872265581723}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "token2Pw_z(tem_learned_pars, id2token, first_probs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "-425819.4926255317\n",
      "-391217.5947833386\n",
      "-390335.4293423235\n",
      "-389370.9419203345\n",
      "-388284.9127037604\n",
      "-387163.9989295611\n",
      "-386108.68397401087\n",
      "-385159.5892519591\n",
      "-384316.224334903\n",
      "-383565.135951779\n",
      "-382893.17425778176\n",
      "-382297.7264387191\n",
      "-381780.42531736754\n",
      "-381339.1619803692\n",
      "-380965.4924255965\n",
      "-380649.1401449335\n",
      "-380380.75739436585\n",
      "-380146.92629415356\n",
      "-379938.0748204201\n",
      "-379751.28196120146\n",
      "-379586.8616816973\n",
      "-379443.37589151104\n",
      "-379317.22124338406\n",
      "-379203.49519475654\n",
      "-379098.2239139739\n",
      "-379000.81620901683\n",
      "-378910.9957664596\n",
      "-378828.7140838424\n",
      "-378754.35189902125\n",
      "-378687.1250606837\n",
      "-378624.75672718964\n",
      "-378566.1002667818\n",
      "-378511.0950549714\n",
      "-378458.853139742\n",
      "-378408.70581985585\n",
      "-378359.372272804\n",
      "-378312.0420342798\n",
      "-378267.78088753414\n",
      "-378225.22952131595\n",
      "-378184.3021556253\n",
      "-378145.1990986536\n",
      "-378105.3688898736\n",
      "-378063.0785134567\n",
      "-378019.60331470566\n",
      "-377980.2499396444\n",
      "-377947.5022322393\n"
     ]
    }
   ],
   "source": [
    "em_learned_pars = TEMsteps(1, 1.00, 0.00, docs2bow, docs2bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(w|z=0) = {'covid': 0.024270215995917794, '19': 0.02365216072391313, 'you': 0.02009835596304525, 'school': 0.013256782456743239, 'year': 0.012749361797425744, 'your': 0.012628073123806665, 'what': 0.012374361802135176, 'children': 0.011592369310568526, 'our': 0.010876614159713508, 'out': 0.010561089965584426, 'as': 0.010139672062222448, 'schools': 0.00998182832315023, 'can': 0.009600616384674462, 'health': 0.009061027215229986, 'be': 0.008949571306068636}\n",
      "\n",
      "P(w|z=1) = {'vaccines': 0.03896814669276154, 'covid': 0.03165215591574358, '19': 0.031203825858483112, 'this': 0.015478098727342984, 'doses': 0.013917198266272917, 'vaccine': 0.013462840697844262, 'first': 0.013196162462808731, 'countries': 0.012771046363391015, 'as': 0.012584793086886856, 'have': 0.012359710933277228, 'unicef': 0.010901370446547537, '#covax': 0.009987636402766863, 'with': 0.009953558324920111, 'today': 0.009660247537424197, 'against': 0.009005049440376079}\n",
      "\n",
      "P(w|z=2) = {'children': 0.025916549719365144, 'unicef': 0.01370419312371283, 'with': 0.013490773939420822, 'health': 0.013018506103759655, '19': 0.01181332781693191, 'water': 0.011536869420883103, 'by': 0.011262591020352047, 'covid': 0.011227302894310922, 'how': 0.01026154709078665, 'this': 0.009813549095328809, 'child': 0.00980865678487449, 'families': 0.00834380735081153, 'their': 0.007974751437643316, 'at': 0.007837662401400143, 'working': 0.007633542532960849}\n",
      "\n",
      "P(w|z=3) = {'children': 0.02217216868586512, 'all': 0.013149694435551895, 'have': 0.011568302981666704, 'our': 0.01151234050441327, 'their': 0.010372506609507552, 'year': 0.009666084658132491, 'been': 0.00924368230768575, 'at': 0.008619839819298453, 'that': 0.008607629478005998, 'water': 0.008325149521545531, 'we': 0.00827243287174858, 'old': 0.008123655926960203, '@unicefchief': 0.007407495672318341, 'violence': 0.006879584506602027, 'lives': 0.006703123050346718}\n",
      "\n",
      "P(w|z=4) = {'we': 0.04339785075593954, '@unicefchief': 0.022742537217963738, 'children': 0.020330973827767523, 'must': 0.014638807436484717, 'need': 0.013682115648525488, 'world': 0.012407033708838767, 'access': 0.011427602072515372, 'pandemic': 0.010911123145389612, 'can': 0.010864197759309278, 'their': 0.009457819734035462, 'support': 0.009235767705280559, 'education': 0.008727212168189669, 'every': 0.008435576885588861, 'people': 0.008429799262625355, 'future': 0.008165870692701855}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "token2Pw_z(em_learned_pars, id2token, first_probs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'deadly': 1, 'wave': 1, 'covid': 1, '19': 1, 'surging': 1, 'nepal': 1, 'much': 2, 'needed': 2, 'medical': 1, 'supplies': 1, 'unicef': 1, 'others': 1, 'have': 1, 'arrived': 1, 'recent': 1, 'weeks': 1, 'but': 1, 'more': 1, 'support': 1, 'save': 1, 'lives': 1, '👉': 1}\n",
      "{'covid': 0.025798001562756627, '19': 0.025501475349848597, 'children': 0.01726045979981774, 'health': 0.016424699514085876, 'as': 0.012124974051717212, 'we': 0.011910178823188133, 'with': 0.011366823740050396, 'their': 0.01057672682376312, 'this': 0.010257751148089975, 'vaccines': 0.009571095883265887, 'need': 0.00951070985231948, 'unicef': 0.009073333589515405, 'our': 0.007567600826086653, 'support': 0.007565807968559286, 'pandemic': 0.0069677644904859145, 'how': 0.006492333725106847, 'has': 0.006401788749532125, 'protect': 0.006291225801196093, 'more': 0.006072502725161493, 'they': 0.00598152630680896, 'must': 0.005794469516482443, 'people': 0.005715443027971143, 'families': 0.005670245464871051, 'have': 0.005646008411085735, 'help': 0.005607664370916557, '@unicefchief': 0.005458037133344319, 'it': 0.005345094213479334, 'now': 0.005096451441248399, 'during': 0.00503558212372515, 'you': 0.00490090256726058, 'mental': 0.004765259500075551, 'your': 0.004686786176548125, 'india': 0.004413138323032072, 'be': 0.004363413711610022, 'by': 0.004173322249189455, 'services': 0.004157997305897897, 'vaccine': 0.004137249694701938, 'most': 0.004082213846180271, '#covid19': 0.003988735899819781, 'safe': 0.003983276830732028, 'can': 0.003979737795065302, 'all': 0.0037818486245038436, 'important': 0.00378053316793751, 'every': 0.0036456057918654128}\n"
     ]
    }
   ],
   "source": [
    "doc2Pw_d(4, tem_learned_pars, id2token, docs2bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'deadly': 1, 'wave': 1, 'covid': 1, '19': 1, 'surging': 1, 'nepal': 1, 'much': 2, 'needed': 2, 'medical': 1, 'supplies': 1, 'unicef': 1, 'others': 1, 'have': 1, 'arrived': 1, 'recent': 1, 'weeks': 1, 'but': 1, 'more': 1, 'support': 1, 'save': 1, 'lives': 1, '👉': 1}\n",
      "{'children': 0.022066839648354698, '19': 0.013985919398151653, 'covid': 0.013655859800531574, 'with': 0.012511277091520511, 'unicef': 0.012385279806845884, 'health': 0.011090333422539393, 'this': 0.010330779815727033, 'by': 0.00976739772130558, 'water': 0.009521062385345813, 'how': 0.009270792905703514, 'child': 0.008226983666709244, 'their': 0.007100656997252054, 'at': 0.006831066725838957, 'families': 0.006667384163975103, 'people': 0.006605730057359597, 'working': 0.006149019789198803, 'young': 0.005972979057476755, 'climate': 0.005661515042242542, 'vaccines': 0.005482015420961938, 'have': 0.005342986250172782, 'has': 0.00518666133428618, 'lives': 0.005173801195348305, 'safe': 0.005168448814537085, 'every': 0.005097268439003931, 'as': 0.00493590169251685, 'about': 0.004827700908949333, 'now': 0.004743586650234911, 'we': 0.0046304094986310135, 'india': 0.004438754793921043, 'more': 0.004431699267592942, 'crisis': 0.0043343695745532975, 'access': 0.0043238289179796975, 'can': 0.004303758316755262, 'help': 0.004276484502521345, '@unicef': 0.004268571415086886, 'amp': 0.004229312342640387, 'during': 0.004151918842632313, 'where': 0.003986594816645747, 'social': 0.003977982793294543, 'children’s': 0.003962580713073414, 'parents': 0.0037592663905341955, 'action': 0.003758611463194504, 'vulnerable': 0.0037331523172414237, 'gaza': 0.003612006487303846}\n"
     ]
    }
   ],
   "source": [
    "doc2Pw_d(4, em_learned_pars, id2token, docs2bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change topic numbers K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2475"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define TEMsteps func again to be compatible with changing topic numbers\n",
    "# just an argument K is added to function inputs\n",
    "def TEMsteps(K, beta_runtimes, first_beta, eta, docs2bow_train, docs2bow_heldout):\n",
    "    beta = first_beta\n",
    "    pars = random_init_pars(K, (len(docs2bow), len(token2id)))\n",
    "    for i in range(beta_runtimes):\n",
    "        print(beta)\n",
    "        new_likeli = likelihood(pars, docs2bow_heldout)\n",
    "        likeli = 2 * new_likeli\n",
    "        while (new_likeli - likeli) / likeli < -0.0001:\n",
    "            likeli = new_likeli\n",
    "            print(likeli)\n",
    "            # one TEM step\n",
    "            prepars = pars\n",
    "            posters = TEstep(beta, pars, docs2bow_train)\n",
    "            pars = TMstep(posters, docs2bow_train)\n",
    "            # print(pars)\n",
    "            new_likeli = likelihood(pars, docs2bow_heldout)\n",
    "        print(new_likeli)\n",
    "        pars = prepars  # undo pars\n",
    "        beta *= eta # new beta\n",
    "    return pars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "-430513.12830310495\n",
      "-391763.5390693877\n",
      "-391284.4763126484\n",
      "-390905.75406712235\n",
      "-390543.5970663452\n",
      "-390199.77735845326\n",
      "-389892.4450339308\n",
      "-389629.84841684834\n",
      "-389409.2997886119\n",
      "-389226.0748744146\n",
      "-389073.9291497022\n",
      "-388944.7253454876\n",
      "-388831.84204608266\n",
      "-388732.2642768752\n",
      "-388644.2559939053\n",
      "-388565.8610729162\n",
      "-388495.0246026755\n",
      "-388430.2726783615\n",
      "-388370.8803717051\n",
      "-388315.82688669226\n",
      "-388263.4012173367\n",
      "-388211.4414686427\n",
      "-388157.9246248506\n",
      "-388102.51112295716\n",
      "-388046.0089056616\n",
      "-387988.70039470855\n",
      "-387932.02301489457\n",
      "-387878.39951969957\n",
      "-387828.5114933154\n",
      "-387780.68568455626\n",
      "-387733.29003234074\n",
      "-387686.41234263324\n",
      "-387640.51409769314\n",
      "-387595.5714967665\n",
      "-387551.20442028233\n",
      "-387508.95022844756\n",
      "-387472.018944851\n"
     ]
    }
   ],
   "source": [
    "em_learned_pars = TEMsteps(2, 1, 1.00, 0.00, docs2bow, docs2bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(w|z=0) = {'covid': 0.021758770844347353, '19': 0.02153851286123545, 'we': 0.013392025450965812, 'vaccines': 0.013178057790070892, '@unicefchief': 0.011251972598036016, 'with': 0.011048746448163262, 'health': 0.010569961046098466, 'how': 0.010317035244652405, 'this': 0.010172914675159675, 'can': 0.009531869855657139, 'as': 0.00915957737799965, 'our': 0.00913525268087066, 'people': 0.009134438220804704, 'children': 0.0090451011069225, 'young': 0.008173632157243156, 'unicef': 0.0073523078355777795, 'have': 0.006890015556415201, 'out': 0.006601635478875142, 'pandemic': 0.006216327861884523, '#covid19': 0.006147663585431794}\n",
      "\n",
      "P(w|z=1) = {'children': 0.020218434627579017, 'covid': 0.012628672163225196, '19': 0.012614058389276489, 'we': 0.011552539656240297, 'year': 0.009143000308086468, 'their': 0.008839372658277693, 'have': 0.008501061728781024, 'education': 0.007677880295079404, 'this': 0.007376809714590481, 'at': 0.007224233738027378, 'be': 0.007197963780074489, 'they': 0.007012676042291227, 'with': 0.006856406030876462, 'old': 0.006794519620774896, 'water': 0.006538587075647768, '”': 0.006243075230072923, 'must': 0.006222352823696362, 'world': 0.006071295445855498, 'more': 0.006060131597867393, 'all': 0.005890096159225674}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "token2Pw_z(em_learned_pars, id2token, first_probs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "metadata": {
   "interpreter": {
    "hash": "689805486acef0e487c075666298695e9f8a8f9c117cf3a598c5305b99d19197"
   }
  },
  "orig_nbformat": 2,
  "vscode": {
   "interpreter": {
    "hash": "397704579725e15f5c7cb49fe5f0341eb7531c82d19f2c29d197e8b64ab5776b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
